A large language model (LLM) is a language model notable for its ability to achieve general-purpose language generation and other natural language processing tasks such as classification. LLMs acquire these abilities by learning statistical relationships from text documents during a computationally intensive self-supervised and semi-supervised training process.[1] LLMs can be used for text generation, a form of generative AI, by taking an input text and repeatedly predicting the next token or word.[2]

LLMs are artificial neural networks. The largest and most capable, as of March 2024, are built with a decoder-only transformer-based architecture while some recent implementations are based on other architectures, such as recurrent neural network variants and Mamba (a state space model).[3][4][5]

Up to 2020, fine tuning was the only way a model could be adapted to be able to accomplish specific tasks. Larger sized models, such as GPT-3, however, can be prompt-engineered to achieve similar results.[6] They are thought to acquire knowledge about syntax, semantics and "ontology" inherent in human language corpora, but also inaccuracies and biases present in the corpora.[7]

Some notable LLMs are OpenAI's GPT series of models (e.g., GPT-3.5 and GPT-4, used in ChatGPT and Microsoft Copilot), Google's PaLM and Gemini (the latter of which is currently used in the chatbot of the same name), xAI's Grok, Meta's LLaMA family of open-source models, Anthropic's Claude models, Mistral AI's open source models, and Databricks' open source DBRX.


Fluent speakers of a language bring an enormous amount of knowledge to bear during comprehension and production. This knowledge is embodied in many forms,
perhaps most obviously in the vocabulary, the rich representations we have of words
and their meanings and usage. This makes the vocabulary a useful lens to explore
the acquisition of knowledge from text, by both people and machines.
Estimates of the size of adult vocabularies vary widely both within and across
languages. For example, estimates of the vocabulary size of young adult speakers of
American English range from 30,000 to 100,000 depending on the resources used
to make the estimate and the definition of what it means to know a word. What
is agreed upon is that the vast majority of words that mature speakers use in their
day-to-day interactions are acquired early in life through spoken interactions with
care givers and peers, usually well before the start of formal schooling. This active
vocabulary is extremely limited compared to the size of the adult vocabulary (usually
on the order of 2000 words for young speakers) and is quite stable, with very few
additional words learned via casual conversation beyond this early stage. Obviously,
this leaves a very large number of words to be acquired by other means.
A simple consequence of these facts is that children have to learn about 7 to 10
words a day, every single day, to arrive at observed vocabulary levels by the time
they are 20 years of age. And indeed empirical estimates of vocabulary growth in
late elementary through high school are consistent with this rate. How do children
achieve this rate of vocabulary growth? Most of this growth is not happening through
direct vocabulary instruction in school, which is not deployed at the rate that would
be required to result in sufficient vocabulary growth.
The most likely explanation is that the bulk of this knowledge acquisition happens as a by-product of reading, as part of the rich processing and reasoning that we
perform when we read. Research into the average amount of time children spend
reading, and the lexical diversity of the texts they read, indicate that it is possible
to achieve the desired rate. But the mechanism behind this rate of learning must
be remarkable indeed, since at some points during learning the rate of vocabulary
growth exceeds the rate at which new words are appearing to the learner!
Many of these facts have motivated approaches to word learning based on the
distributional hypothesis, introduced in Chapter 6. This is the idea that something
about what we’re loosely calling word meanings can be learned even without any
grounding in the real world, solely based on the content of the texts we encounter
over our lives. This knowledge is based on the complex association of words with
the words they co-occur with (and with the words that those words occur with).
The crucial insight of the distributional hypothesis is that the knowledge that we
acquire through this process can be brought to bear long after its initial acquisition.
2 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
Of course, adding grounding from vision or from real-world interaction can help
build even more powerful models, but even text alone is remarkably useful.
pretraining In this chapter we formalize this idea of pretraining—learning knowledge about
language and the world from vast amounts of text—and call the resulting pretrained
language models large language models. Large language models exhibit remarkable performance on all sorts of natural language tasks because of the knowledge
they learn in pretraining, and they will play a role throughout the rest of this book.
They have been especially transformative for tasks where we need to produce text,
like summarization, machine translation, question answering, or chatbots.
transformer The standard architecture for building large language models is the transformer.
We thus begin this chapter by introducing this architecture in detail. The transformer
makes use of a novel mechanism called self-attention, which developed out of the
idea of attention that was introduced for RNNs in Chapter 9. Self-attention can
be thought of a way to build contextual representations of a word’s meaning that
integrate information from surrounding words, helping the model learn how words
relate to each other over large spans of text.
We’ll then see how to apply the transformer to language modeling, in a setting often called causal or autoregressive language models, in which we iteratively predict
words left-to-right from earlier words. These language models, like the feedforward
and RNN language models we have already seen, are thus self-trained: given a large
corpus of text, we iteratively teach the model to guess the next word in the text from
the prior words. In addition to training, we’ll introduce algorithms for generating
texts, including important methods like greedy decoding, beam search, and sampling. And we’ll talk about the components of popular large language models like
the GPT family.
Finally, we’ll see the great power of language models: almost any NLP task
can be modeled as word prediction, if we think about it in the right way. We’ll
work through an example of using large language models to solve one NLP task
of summarization (generating a short text that summarizes some larger document).
The use of a large language model to generate text is one of the areas in which the
impact of the last decade of neural algorithms for NLP has been the largest. Indeed,
text generation, along with image generation and code generation, constitute a new
area of AI that is often called generative AI.
We’ll save three more areas of large language models for the next three chapters;
Chapter 11 will introduce the bidirectional transformer encoder and the method of
masked language modeling, used for the popular BERT family of models. Chapter 12 will introduce the most powerful way to interact with large language models:
prompting them to perform other NLP tasks by simply giving directions or instructions in natural language to a transformer that is pretrained on language modeling.
And Chapter 13 will introduce the use of the encoder-decoder architecture for transformers in the context of machine translation.
10.1 The Transformer: A Self-Attention Network
transformer In this section we introduce the architecture of the transformer, the algorithm that
underlies most modern NLP systems. When used for causal language modeling, the
input to a transformer is a sequence of words, and the output is a prediction for what
word comes next, as well as a sequence of contextual embedding that represents
the contextual meaning of each of the input words. Like the LSTMs of Chapter 9,
10.1 • THE TRANSFORMER: A SELF-ATTENTION NETWORK 3
transformers are a neural architecture that can handle distant information. But unlike
LSTMs, transformers are not based on recurrent connections (which can be hard to
parallelize), which means that transformers can be more efficient to implement at
scale.
Transformers are made up of stacks of transformer blocks, each of which is a
multilayer network that maps sequences of input vectors (x1,...,xn) to sequences of
output vectors (z1,...,zn) of the same length. These blocks are made by combinself-attention ing simple linear layers, feedforward networks, and self-attention layers, the key
innovation of transformers. Self-attention allows a network to directly extract and
use information from arbitrarily large contexts. We’ll start by describing how selfattention works and then return to how it fits into larger transformer blocks. Finally,
we’ll describe how to use the transformer block together with some input and output
mechanisms as a language model, to predict upcoming words from prior words in
the context.
10.1.1 Transformers: the intuition
The intuition of a transformer is that across a series of layers, we build up richer and
richer contextualized representations of the meanings of input words or tokens (we
will refer to the input as a sequence of words for convenience, although technically
the input is first tokenized by an algorithm like BPE, so it is a series of tokens rather
than words). At each layer of a transformer, to compute the representation of a
word i we combine information from the representation of i at the previous layer
with information from the representations of the neighboring words. The goal is to
produce a contextualized representation for each word at each position. We can think
of these representations as a contextualized version of the static vectors we saw in
Chapter 6, which each represented the meaning of a word type. By contrast, our goal
in transformers is to produce a contextualized version, something that represents
what this word means in the particular context in which it occurs.
We thus need a mechanism that tells us how to weigh and combine the representations of the different words from the context at the prior level in order to compute
our representation at this layer. This mechanism must be able to look broadly in the
context, since words have rich linguistic relationships with words that can be many
sentences away. Even within the sentence, words have important linguistic relationships with contextual words. Consider these examples, each exhibiting linguistic
relationships that we’ll discuss in more depth in later chapters:
(10.1) The keys to the cabinet are on the table.
(10.2) The chicken crossed the road because it wanted to get to the other side.
(10.3) I walked along the pond, and noticed that one of the trees along the bank
had fallen into the water after the storm.
In (10.1), the phrase The keys is the subject of the sentence, and in English and
many languages, must agree in grammatical number with the verb are; in this case
both are plural. In English we can’t use a singular verb like is with a plural subject like keys; we’ll discuss agreement more in Chapter 17. In (10.2), the pronoun
it corefers to the chicken; it’s the chicken that wants to get to the other side. We’ll
discuss coreference more in Chapter 26. In (10.3), the way we know that bank refers
to the side of a pond or river and not a financial institution is from the context, including words like pond and water. We’ll discuss word senses more in Chapter 23.
These helpful contextual words can be quite far way in the sentence or paragraph,
4 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
so we need a mechanism that can look broadly in the context to help compute representations for words.
Self-attention is just such a mechanism: it allows us to look broadly in the context and tells us how to integrate the representation from words in that context from
layer k −1 to build the representation for words in layer k.
 The self-attention weight distribution α that is part of the computation of the
representation for the word it at layer 6. In computing the representation for it, we attend
differently to the various words at layer 5, with darker shades indicating higher self-attention
values. Note that the transformer is attending highly to animal, a sensible result, since in this
example it corefers with the animal, and so we’d like the representation for it to draw on the
representation for animal. Figure simplified from (Uszkoreit, 2017).
Fig. 10.1 shows an schematic example simplified from a real transformer (Uszkoreit, 2017). Here we want to compute a contextual representation for the word it, at
layer 6 of the transformer, and we’d like that representation to draw on the representations of all the prior words, from layer 5. The figure uses color to represent the
attention distribution over the contextual words: the word animal has a high attention weight, meaning that as we are computing the representation for it, we will draw
most heavily on the representation for animal. This will be useful for the model to
build a representation that has the correct meaning for it, which indeed is coreferent here with the word animal. (We say that a pronoun like it is coreferent with a
noun like animal if they both refer to the same thing; we’ll return to coreference in
Chapter 26.)
10.1.2 Causal or backward-looking self-attention
The concept of context can be used in two ways in self-attention. In causal, or
backward looking self-attention, the context is any of the prior words. In general
bidirectional self-attention, the context can include future words. In this chapter
we focus on causal, backward looking self-attention; we’ll introduce bidirectional
self-attention in Chapter 11.
Fig. 10.2 thus illustrates the flow of information in a single causal, or backward
looking, self-attention layer. As with the overall transformer, a self-attention layer
maps input sequences (x1,...,xn) to output sequences of the same length (a1,...,an).
When processing each item in the input, the model has access to all of the inputs
up to and including the one under consideration, but no access to information about
inputs beyond the current one. In addition, the computation performed for each item
is independent of all the other computations. The first point ensures that we can use
this approach to create language models and use them for autoregressive generation,
and the second point means that we can easily parallelize both forward inference
and training of such models.
10.1 • THE TRANSFORMER: A SELF-ATTENTION NETWORK 5
 Information flow in a causal (or masked) self-attention model. In processing
each element of the sequence, the model attends to all the inputs up to, and including, the
current one. Unlike RNNs, the computations at each time step are independent of all the
other steps and therefore can be performed in parallel.
10.1.3 Self-attention more formally
We’ve given the intuition of self-attention (as a way to compute representations of a
word at a given layer by integrating information from words at the previous layer)
and we’ve defined context as all the prior words in the input. Let’s now introduce
the self-attention computation itself.
The core intuition of attention is the idea of comparing an item of interest to a
collection of other items in a way that reveals their relevance in the current context.
In the case of self-attention for language, the set of comparisons are to other words
(or tokens) within a given sequence. The result of these comparisons is then used to
compute an output sequence for the current input sequence. For example, returning
to Fig. 10.2, the computation of a3 is based on a set of comparisons between the
input x3 and its preceding elements x1 and x2, and to x3 itself.
How shall we compare words to other words? Since our representations for
words are vectors, we’ll make use of our old friend the dot product that we used
for computing word similarity in Chapter 6, and also played a role in attention in
Chapter 9. Let’s refer to the result of this comparison between words i and j as a
score
The result of a dot product is a scalar value ranging from −∞ to ∞, the larger
the value the more similar the vectors that are being compared. Continuing with our
example, the first step in computing y3 would be to compute three scores: x3 · x1,
x3 · x2 and x3 · x3. Then to make effective use of these scores, we’ll normalize them
with a softmax to create a vector of weights, αi j, that indicates the proportional
relevance of each input to the input element i that is the current focus of attention.

Of course, the softmax weight will likely be highest for the current focus element
i, since vecxi
is very similar to itself, resulting in a high dot product. But other
context words may also be similar to i, and the softmax will also assign some weight
to those words.
Given the proportional scores in α, we generate an output value ai by summing
6 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
the inputs seen so far, each weighted by its α value.

The steps embodied in Equations 10.4 through 10.7 represent the core of an
attention-based approach: a set of comparisons to relevant items in some context,
a normalization of those scores to provide a probability distribution, followed by a
weighted sum using this distribution. The output a is the result of this straightforward computation over the inputs.
This kind of simple attention can be useful, and indeed we saw in Chapter 9
how to use this simple idea of attention for LSTM-based encoder-decoder models
for machine translation. But transformers allow us to create a more sophisticated
way of representing how words can contribute to the representation of longer inputs.
Consider the three different roles that each input embedding plays during the course
of the attention process.
• As the current focus of attention when being compared to all of the other
query preceding inputs. We’ll refer to this role as a query.
• In its role as a preceding input being compared to the current focus of attenkey tion. We’ll refer to this role as a key.
value • And finally, as a value used to compute the output for the current focus of
attention.
To capture these three different roles, transformers introduce weight matrices
WQ, WK, and WV. These weights will be used to project each input vector xi
into
a representation of its role as a key, query, or value.
qi = xiWQ; ki = xiWK; vi = xiWV
(10.8)
The inputs x and outputs y of transformers, as well as the intermediate vectors after
the various layers like the attention output vector a, all have the same dimensionality
1 × d. We’ll have a dimension dk for the key and query vectors, and a separate
dimension dv for the value vectors. In the original transformer work (Vaswani et al.,
2017), d was 512, dk and dv were both 64.
Figure 10.6 A transformer block showing all the layers.
Feedforward layer The feedforward layer contains N position-wise networks, one
at each position. Each is a fully-connected 2-layer network, i.e., one hidden layer,
two weight matrices, as introduced in Chapter 7. The weights are the same for each
position, but the parameters are different from layer to layer. Unlike attention, the
feedforward networks are independent for each position and so can be computed in
parallel. It is common to make the dimensionality dff of the hidden layer of the
feedforward network be larger than the model dimensionality d. (For example in the
original transformer model, d = 512 and dff = 2048.)
Residual connections Residual connections are connections that pass information from a lower layer to a higher layer without going through the intermediate
10.3 • TRANSFORMER BLOCKS 11
layer. Allowing information from the activation going forward and the gradient going backwards to skip a layer improves learning and gives higher level layers direct
access to information from lower layers (He et al., 2016). Residual connections in
transformers are implemented simply by adding a layer’s input vector to its output vector before passing it forward. In the transformer block shown in Fig. 10.6,
residual connections are used with both the attention and feedforward sublayers.
Layer Norm These summed vectors are then normalized using layer normalizalayer norm tion (Ba et al., 2016). Layer normalization (usually called layer norm) is one of
many forms of normalization that can be used to improve training performance in
deep neural networks by keeping the values of a hidden layer in a range that facilitates gradient-based training. Layer norm is a variation of the standard score, or
z-score, from statistics applied to a single vector in a hidden layer. The input to
layer norm is a single vector, for a particular token position i, and the output is that
vector normalized. Thus layer norm takes as input a single vector of dimensionality
d and produces as output a single vector of dimensionality d. The first step in layer
normalization is to calculate the mean, µ, and standard deviation, σ, over the elements of the vector to be normalized. Given a hidden layer with dimensionality dh,

Given these values, the vector components are normalized by subtracting the mean
from each and dividing by the standard deviation. The result of this computation is
a new vector with zero mean and a standard deviation of one.

Finally, in the standard implementation of layer normalization, two learnable parameters, γ and β, representing gain and offset values, are introduced.
LayerNorm = γ ˆx+β (10.23)
Putting it all together The function computed by a transformer block can be expressed as:
O = LayerNorm(X+SelfAttention(X)) (10.24)
H = LayerNorm(O+FFN(O)) (10.25)
Or we can break it down with one equation for each component computation, using

12 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
Crucially, the input and output dimensions of transformer blocks are matched so
they can be stacked. Each token xi at the input to the block has dimensionality d,
and so the input X and output H are both of shape [N ×d].
Transformers for large language models stack many of these blocks, from 12
layers (used for the T5 or GPT-3-small language models) to 96 layers (used for
GPT-3 large), to even more for more recent models. We’ll come back to this issues
of stacking in a bit.
10.4 The Residual Stream view of the Transformer Block
The previous sections viewed the transformer block as applied to the entire N-token
input X of shape [N ×d], producing an output also of shape [N ×d].
While packing everything this way is a computationally efficient way to implement the transformer block, it’s not always the most perspicuous way to understand
what the transformer is doing. It’s often clearer to instead visualize what is happening to an individual token vector xi
in the input as it is processed through each
transformer block. After all, most of the components of the transformer are designed to take a single vector of dimensionality d, corresponding to a single token,
and produce an output vector also of dimensionality d. For example, the feedforward layer takes a single d-dimensional vector and produces a single d-dimensional
vector. Over the N tokens in a batch, we simply use the identical feedforward layer
weights (W1, W2, b1 and b2) for each token i. Similarly, the layer norm function takes
a single d-dimensional vector and produces a normalized d-dimensional version. showing how the input to the transformer
block xi
is passed up through residual connections, the output of the feedforward and multihead attention layers are added in, and processed by layer norm, to produce the output of
this block, hi
, which is used as the input to the next layer transformer block. Note that of all
the components, only the MultiHeadAttention component reads information from the other
residual streams in the context.
We can therefore talk about the processing of an individual token through all
10.4 • THE RESIDUAL STREAM VIEW OF THE TRANSFORMER BLOCK 13
residual stream these layers as a stream of d-dimensional representations, called the residual stream
and visualized in Fig. 10.7. The input at the bottom of the stream is an embedding
for a token, which has dimensionality d. That initial embedding is passed up by the
residual connections and the outputs of feedforward and attention layers get added
into it. For each token i, at each block and layer we are passing up an embedding
of shape [1 × d]. The residual layers are constantly copying information up from
earlier embeddings (hence the metaphor of ‘residual stream’), so we can think of the
other components as adding new views of this representation back into this constant
stream. Feedforward networks add in a different view of the earlier embedding.

Notice that the only component that takes as input information from other tokens
(other residual streams) is multi-head attention, which (as we see from (10.32) looks
at all the neighboring tokens in the context. The output from attention, however,
is then added into to this token’s embedding stream. In fact, Elhage et al. (2021)
show that we can view attention heads as literally moving attention from the residual stream of a neighboring token into the current stream. The high-dimensional
embedding space at each position thus contains information about the current token and about neighboring tokens, albeit in different subspaces of the vector space.
Fig. 10.8 shows a visualization of this movement.
Token A
residual stream
Token B
residual stream
Figure 10.8 An attention head can move information from token A’s residual stream into
token B’s residual stream.
Equation (10.32) and following are just just the equation for a single transformer
block, but the residual stream metaphor goes through all the transformer layers,
from the first transformer blocks to the 12th, in a 12-layer transformer. At the earlier
transformer blocks, the residual stream is representing the current token. At the
highest transformer blocks, the residual stream is usual representing the following
token, since at the very end it’s being trained to predict the next token.
Pre-norm vs. post-norm architecture There is an alternative form of the transformer architecture that is commonly used because it performs better in many cases.
In this prenorm transformer architecture, the layer norm happens in a slightly dif- prenorm
transformer
14 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
ferent place: before the attention layer and before the feedforward layer, rather than
afterwards. Fig. 10.9 shows this architecture, with the equations below:

Figure 10.9 The architecture of the prenorm transformer block. Here the nature of the
residual stream, passing up information from the input, is even clearer.
The prenorm transformer has one extra requirement: at the very end of the last
(highest) transformer block, there is a single extra layer norm that is run on the last hi
of each token stream (just below the language model head layer that we will define
below).
10.5 The input: embeddings for token and position
Let’s talk about where the input X comes from. Given a sequence of N tokens (N is
embedding the context length in tokens), the matrix X of shape [N ×d] has an embedding for
each word in the context. The transformer does this by separately computing two
embeddings: an input token embedding, and an input positional embedding.
A token embedding, introduced in Chapter 7 and Chapter 9, is a vector of dimension d that will be our initial representation for the input token. (As we pass
vectors up through the transformer layers in the residual stream, this embedding
representation will change and grow, incorporating context and playing a different
10.5 • THE INPUT: EMBEDDINGS FOR TOKEN AND POSITION 15
role depending on the kind of language model we are building.) The set of initial
embeddings are stored in the embedding matrix E, which has a row for each of the
|V| tokens in the vocabulary. Thus each each word is a row vector of d dimensions,
and E has shape [|V| ×d].
Given an input token string like Thanks for all the we first convert the tokens
into vocabulary indices (these were created when we first tokenized the input using
BPE or SentencePiece). So the representation of thanks for all the might be w =
[5,4000,10532,2224]. Next we use indexing to select the corresponding rows from
E, (row 5, row 4000, row 10532, row 2224).
Another way to think about selecting token embeddings from the embedding
matrix is to represent tokens as one-hot vectors of shape [1 × |V|], i.e., with one
one-hot vector dimension for each word in the vocabulary. Recall that in a one-hot vector all the
elements are 0 except one, the element whose dimension is the word’s index in the
vocabulary, which has value 1. So if the word “thanks” has index 5 in the vocabulary,

Figure 10.11 Selecting the embedding matrix for the input sequence of token ids W by
multiplying a one-hot matrix corresponding to W by the embedding matrix E.
These token embeddings are not position-dependent. To represent the position
of each token in the sequence, we combine these token embeddings with positional
embeddings specific to each position in an input sequence. positional
embeddings
Where do we get these positional embeddings? The simplest method, called
absolute position, is to start with randomly initialized embeddings corresponding absolute
position
to each possible input position up to some maximum length. For example, just as
we have an embedding for the word fish, we’ll have an embedding for the position 3.
16 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
As with word embeddings, these positional embeddings are learned along with other
parameters during training. We can store them in a matrix Epos of shape [1timesN].
To produce an input embedding that captures positional information, we just add
the word embedding for each input to its corresponding positional embedding. The
individual token and position embeddings are both of size [1×d], so their sum is also
[1 × d], This new embedding serves as the input for further processing. Fig. 10.12
shows the idea.

Figure 10.12 A simple way to model position: add an embedding of the absolute position
to the token embedding to produce a new embedding of the same dimenionality.
The final representation of the input, the matrix X, is an [N ×d] matrix in which
each row i is the representation of the ith token in the input, computed by adding
E[id(i)]—the embedding of the id of the token that occurred at position i—, to P[i],
the positional embedding of position i.
A potential problem with the simple absolute position embedding approach is
that there will be plenty of training examples for the initial positions in our inputs
and correspondingly fewer at the outer length limits. These latter embeddings may
be poorly trained and may not generalize well during testing. An alternative approach to absolute positional embeddings is to choose a static function that maps
integer inputs to real-valued vectors in a way that captures the inherent relationships among the positions. That is, it captures the fact that position 4 in an input is
more closely related to position 5 than it is to position 17. A combination of sine
and cosine functions with differing frequencies was used in the original transformer
work. Even more complex positional embedding methods exist, such as ones that
represent relative position instead of absolute position, often implemented in the
attention mechanism at each layer rather than being added once at the initial input.
10.6 The Language Modeling Head
The last component of the transformer we must introduce is the language modeling
head. When we apply pretrained transformer models to various tasks, we use the language
modeling head
head term head to mean the additional neural circuitry we add on top of the basic transformer architecture to enable that task. The language modeling head is the circuitry
we need to do language modeling.
Recall that language models, from the simple n-gram models of Chapter 3 through
the feedforward and RNN language models of Chapter 7 and Chapter 9, are word
predictors. Given a context of words, they assign a probability to each possible next
10.6 • THE LANGUAGE MODELING HEAD 17
word. For example, if the preceding context is “Thanks for all the” and we want to
know how likely the next word is “fish” we would compute:
P(fish|Thanks for all the)
Language models give us the ability to assign such a conditional probability to every
possible next word, giving us a distribution over the entire vocabulary. The n-gram
language models of Chapter 3 compute the probability of a word given counts of
its occurrence with the n − 1 prior words. The context is thus of size n − 1. For
transformer language models, the context is the size of the transformer’s context
window, which can be quite large: up to 2048 or even 4096 tokens for large models.
The job of the language modeling head is to take the the output of the final
transformer layer from the last token N and use it to predict the upcoming word at
position N +1. Fig. 10.13 shows how to accomplish this task, taking the output of
the last token at the last layer (the d-dimensional output embedding of shape [1×d])
and producing a probability distribution over words (from which we will choose one
to generate).

Figure 10.14 A final transformer decoder-only model, stacking post-norm transformer
blocks and mapping from a set of input tokens w1 to wN to a predicted next word wN+1.
Fig. 10.14 shows the total stacked architecture. Note that the input to the first
transformer block is represented as X, which is the N indexed word embeddings +
position embeddings, E[w] +P), but the input to all the other layers is the output H
from the layer just below the current one).
Now that we see all these transformer layers spread out on the page, we can point
out another useful feature of the unembedding layer: as a tool for interpretability of
10.7 • LARGE LANGUAGE MODELS WITH TRANSFORMERS 19
logit lens the internals of the transformer that we call the logit lens (Nostalgebraist, 2020).
We can take a vector from any layer of the transformer and, pretending that it is
the prefinal embedding, simply multiply it by the unembedding layer to get logits,
and compute a softmax to see the distribution over words that that vector might
be representing. This can be a useful window into the internal representations of
the model. Since the network wasn’t trained to make the internal representations
function in this way, the logit lens doesn’t always work perfectly, but this can still
be a useful trick.
Anyhow, the Fig. 10.14 thus sketches out the entire process of taking a series of
words w1 ...wN and using the model to predict the next word wN+1.
A terminological note before we conclude: You will sometimes see a transformer used for this kind of unidirectional causal language model called a decoderonly model. This is because this model constitutes roughly half of the encoder- decoder-only
model
decoder model for transformers that we’ll see how to apply to machine translation
in Chapter 13. (Confusingly, the original introduction of the transformer had an
encoder-decoder architecture, and it was only later that the standard paradigm for
causal language model was defined by using only the decoder part of this original
architecture).
In the next sections we’ll introduce what kind of tasks large language models can
be used for, discuss various generation methods for sampling possible next words,
and show how to train a transformer-based large language model. In the following chapters we’ll expand on these ideas to introduce fine-tuning, prompting, and
encoder-decoder architectures for transformer-based large language models.
10.7 Large Language Models with Transformers
We’ve now seen most of the components of a transformer for language modeling
(what remains is sampling and training, which we’ll get to in the following sections). Before we do that, we use this section to talk about why and how we apply
transformer-based large language models to NLP tasks.
All of these tasks are cases of conditional generation, the task of generating text
conditioned on an input piece of text, a prompt. The fact that transformers have such
long contexts (1024 or even 4096 tokens) makes them very powerful for conditional
generation, because they can look back so far into the prompting text.
Consider the simple task of text completion, illustrated in Fig. 10.15. Here a
language model is given a text prefix and is asked to generate a possible completion.
Note that as the generation process proceeds, the model has direct access to the
priming context as well as to all of its own subsequently generated outputs (at least
as much as fits in the large context window).. This ability to incorporate the entirety
of the earlier context and generated outputs at each time step is the key to the power
of large language models built from transformers.
So why should we care about predicting upcoming words? The insight of large
language modeling is that many practical NLP tasks can be cast as word prediction, and that a powerful-enough language model can solve them with a high degree
of accuracy. For example, we can cast sentiment analysis as language modeling by
giving a language model a context like:
The sentiment of the sentence “I like Jackie Chan” is:
and comparing the following conditional probability of the words “positive” and the
20 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
Prefix Text
Completion Text
Input
Embeddings
Transformer
Blocks
Sample from Softmax
So long
all
and thanks for all
the
the
…
linear layer
Figure 10.15 Autoregressive text completion with transformer-based large language models.
word “negative” to see which is higher:
P(positive|The sentiment of the sentence “I like Jackie Chan” is:)
P(negative|The sentiment of the sentence “I like Jackie Chan” is:)
If the word “positive” is more probable, we say the sentiment of the sentence is
positive, otherwise we say the sentiment is negative.
We can also cast more complex tasks as word prediction. Consider the task
of answering simple questions, a task we return to in Chapter 14. In this task the
system is given some question and must give a textual answer. We can cast the task
of question answering as word prediction by giving a language model a question and
a token like A: suggesting that an answer should come next:
Q: Who wrote the book ‘‘The Origin of Species"? A:
If we ask a language model to compute
P(w|Q: Who wrote the book “The Origin of Species”? A:)
and look at which words w have high probabilities, we might expect to see that
Charles is very likely, and then if we choose Charles and continue and ask
P(w|Q: Who wrote the book “The Origin of Species”? A: Charles)
we might now see that Darwin is the most probable word, and select it.
Conditional generation can even be used to accomplish tasks that must generate
longer responses. Consider the task of text summarization, which is to take a long text
summarization
text, such as a full-length article, and produce an effective shorter summary of it.
We can cast summarization as language modeling by giving a large language model
a text, and follow the text by a token like tl;dr; this token is short for something
like ‘too long; don’t read’ and in recent years people often use this token, especially
in informal work emails, when they are going to give a short summary. We can
then do conditional generation: give the language model this prefix, and then ask
10.7 • LARGE LANGUAGE MODELS WITH TRANSFORMERS 21
it to generate the following words, one by one, and take the entire response as a
summary. Fig. 10.16 shows an example of a text and a human-produced summary
from a widely-used summarization corpus consisting of CNN and Daily Mirror news
articles.
Original Article
The only thing crazier than a guy in snowbound Massachusetts boxing up the powdery white stuff
and offering it for sale online? People are actually buying it. For $89, self-styled entrepreneur
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says.
But not if you live in New England or surrounding states. “We will not ship snow to any states
in the northeast!” says Waring’s website, ShipSnowYo.com. “We’re in the business of expunging
snow!”
His website and social media accounts claim to have filled more than 133 orders for snow – more
than 30 on Tuesday alone, his busiest day yet. With more than 45 total inches, Boston has set a
record this winter for the snowiest month in its history. Most residents see the huge piles of snow
choking their yards and sidewalks as a nuisance, but Waring saw an opportunity.
According to Boston.com, it all started a few weeks ago, when Waring and his wife were shoveling deep snow from their yard in Manchester-by-the-Sea, a coastal suburb north of Boston.
He joked about shipping the stuff to friends and family in warmer states, and an idea was born.
His business slogan: “Our nightmare is your dream!” At first, ShipSnowYo sold snow packed
into empty 16.9-ounce water bottles for $19.99, but the snow usually melted before it reached its
destination...
Summary
Kyle Waring will ship you 6 pounds of Boston-area snow in an insulated Styrofoam box – enough
for 10 to 15 snowballs, he says. But not if you live in New England or surrounding states.
Figure 10.16 Examples of articles and summaries from the CNN/Daily Mail corpus (Hermann et al., 2015),
(Nallapati et al., 2016).
If we take this full article and append the token tl;dr, we can use this as
the context to prime the generation process to produce a summary as illustrated
in Fig. 10.17. Again, what makes transformers able to succeed at this task (as
compared, say, to the primitive n-gram language model) is that the ability of selfattention to incorporate information from the large context windows means that
the model has access to the original article as well as to the newly generated text
throughout the process.
Which words do we generate at each step? One simple way to generate words
is to always generate the most likely word given the context. Generating the most
likely word given the context is called greedy decoding. A greedy algorithm is one greedy
decoding
that make a choice that is locally optimal, whether or not it will turn out to have
been the best choice with hindsight. Thus in greedy decoding, at each time step in
generation, the output yt
is chosen by computing the probability for each possible
outputs (every word in the vocabulary) and then choosing the highest probability
word (the argmax):
wˆt = argmaxw∈V P(w|w<t) (10.46)
In practice, however, we don’t use greedy decoding with large language models.
A major problem with greedy decoding is that because the words it chooses are (by
definition) extremely predictable, the resulting text is generic and often quite repetitive. Indeed, greedy decoding is so predictable that it is deterministic; if the context
22 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
Original Story
Generated Summary
… reached
Kyle
its destination Kyle
Waring
The only Waring
…
will
Delimiter
tl;dr will
Figure 10.17 Summarization with large language models using the tl;dr token and context-based autoregressive generation.
is identical, and the probabilistic model is the same, greedy decoding will always result in generating exactly the same string. We’ll see in Chapter 13 that an extension
to greedy decoding called beam search works well in tasks like machine translation,
which are very constrained in that we are always generating a text in one language
conditioned on a very specific text in another language. In most other tasks, however, people prefer text which has been generated by more sophisticated methods,
called sampling methods, that introduce a bit more diversity into the generations.
We’ll see how to do that in the next few sections.
10.8 Large Language Models: Generation by Sampling
The core of the generation process for large language models is the task of choosing
the single word to generate next based on the context and based on the probabilities
that the model assigns to possible words. This task of choosing a word to generate
decoding based on the model’s probabilities is called decoding. Decoding from a language
model in a left-to-right manner (or right-to-left for languages like Arabic in which
we read from right to left), and thus repeatedly choosing the next word conditioned
on our previous choices is called autoregressive generation or causal LM genera- autoregressive
generation
tion.
1
(As we’ll see, alternatives like the masked language models of Chapter 11 are
non-causal because they can predict words based on both past and future words).
The most common method for decoding in large language models is sampling.
sampling Recall from Chapter 3 that sampling from a model’s distribution over words means
to choose random words according to their probability assigned by the model. That
is, we iteratively choose a word to generate according to its probability in context
1 Technically an autoregressive model predicts a value at time t based on a linear function of the values
at times t −1, t −2, and so on. Although language models are not linear (since they have many layers of
non-linearities), we loosely refer to this generation technique as autoregressive since the word generated
at each time step is conditioned on the word selected by the network from the previous step.
10.8 • LARGE LANGUAGE MODELS: GENERATION BY SAMPLING 23
as defined by the model. Thus we are more likely to generate words that the model
thinks have a high probability in the context and less likely to generate words that
the model thinks have a low probability.
We saw back in Chapter 3 on page ?? how to generate text from a unigram language model , by repeatedly randomly sampling words according to their probability
until we either reach a pre-determined length or select the end-of-sentence token. To
generate text from a trained transformer language model we’ll just generalize this
model a bit: at each step we’ll sample words according to their probability conditioned on our previous choices, and we’ll use a transformer language model as the
probability model that tells us this probability.
We can formalize this algorithm for generating a sequence of words W = w1,w2,...,wN
until we hit the end-of-sequence token, using x ∼ p(x) to mean ‘choose x by sampling from the distribution p(x):
i←1
wi ∼ p(w)
while wi != EOS
i←i + 1
wi ∼ p(wi
| w<i)
The algorithm above is called random sampling, and it turns out random sam- random
sampling
pling doesn’t work well enough. The problem is that even though random sampling
is mostly going to generate sensible, high-probable words, there are many odd, lowprobability words in the tail of the distribution, and even though each one is lowprobability, if you add up all the rare words, they constitute a large enough portion
of the distribution that they get chosen often enough to result in generating weird
sentences. For this reason, instead of random sampling, we usually use sampling
methods that avoid generating the very unlikely words.
The sampling methods we introduce below each have parameters that enable
trading off two important factors in generation: quality and diversity. Methods
that emphasize the most probable words tend to produce generations that are rated
by people as more accurate, more coherent, and more factual, but also more boring
and more repetitive. Methods that give a bit more weight to the middle-probability
words tend to be more creative and more diverse, but less factual and more likely to
be incoherent or otherwise low-quality.
10.8.1 Top-k sampling
top-k sampling Top-k sampling is a simple generalization of greedy decoding. Instead of choosing
the single most probable word to generate, we first truncate the distribution to the
top k most likely words, renormalize to produce a legitimate probability distribution,
and then randomly sample from within these k words according to their renormalized
probabilities. More formally:
1. Choose in advance a number of words k
2. For each word in the vocabulary V, use the language model to compute the
likelihood of this word given the context p(wt
|w<t)
3. Sort the words by their likelihood, and throw away any word that is not one of
the top k most probable words.
4. Renormalize the scores of the k words to be a legitimate probability distribution.
24 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
5. Randomly sample a word from within these remaining k most-probable words
according to its probability.
When k = 1, top-k sampling is identical to greedy decoding. Setting k to a larger
number than 1 leads us to sometimes select a word which is not necessarily the most
probable, but is still probable enough, and whose choice results in generating more
diverse but still high-enough-quality text.
10.8.2 Nucleus or top-p sampling
One problem with top-k sampling is that k is fixed, but the shape of the the probability distribution over words differs in different contexts. If we set k = 10, sometimes
the top 10 words will be very likely and include most of the probability mass, but
other times the probability distribution will be flatter and the top 10 words will only
include a small part of the probability mass.
top-p sampling An alternative, called top-p sampling or nucleus sampling (Holtzman et al.,
2020), is to keep not the top k words, but the top p percent of the probability mass.
The goal is the same; to truncate the distribution to remove the very unlikely words.
But by measuring probability rather than the number of words, the hope is that the
measure will be more robust in very different contexts, dynamically increasing and
decreasing the pool of word candidates.
Given a distribution P(wt
|w<t), the top-p vocabulary V
(p)
is the smallest set of
words such that
X
w∈V
(p)
P(w|w<t) ≥ p. (10.47)
10.8.3 Temperature sampling
In temperature sampling, we don’t truncate the distribution, but instead reshape temperature
sampling
it. The intuition for temperature sampling comes from thermodynamics, where a
system at a high temperature is very flexible and can explore many possible states,
while a system at a lower temperature is likely to explore a subset of lower energy
(better) states. In low-temperature sampling, we smoothly increase the probability
of the most probable words and decrease the probability of the rare words.
We implement this intuition by simply dividing the logit by a temperature parameter τ before we normalize it by passing it through the softmax. In low-temperature
sampling, τ ∈ (0,1]. Thus instead of computing the probability distribution over the
vocabulary directly from the logit as in the following (repeated from (10.45):
y = softmax(u) (10.48)
we instead first divide the logits by τ, computing the probability vector y as
y = softmax(u/τ) (10.49)
Why does this work? When τ is close to 1 the distribution doesn’t change much.
But the lower τ is, the larger the scores being passed to the softmax (dividing by a
smaller fraction τ ≤ 1 results in making each score larger). Recall that one of the
useful properties of a softmax is that it tends to push high values toward 1 and low
values toward 0. Thus when larger numbers are passed to a softmax the result is
a distribution with increased probabilities of the most high-probability words and
decreased probabilities of the low probability words, making the distribution more
greedy. As τ approaches 0 the probability of the most likely word approaches 1.
10.9 • LARGE LANGUAGE MODELS: TRAINING TRANSFORMERS 25
Note, by the way, that there can be other situations where we may want to do
something quite different and flatten the word probability distribution instead of
making it greedy. Temperature sampling can help with this situation too, in this case
high-temperature sampling, in which case we use τ > 1.
10.9 Large Language Models: Training Transformers
How do we teach a transformer to be a language model? What is the algorithm and
what data do we train on?
10.9.1 Self-supervised training algorithm
self-supervision To train a transformer as a language model, we use the same self-supervision (or
self-training) algorithm we saw in Section ??: we take a corpus of text as training
material and at each time step t ask the model to predict the next word. We call
such a model self-supervised because we don’t have to add any special gold labels
to the data; the natural sequence of words is its own supervision! We simply train the
model to minimize the error in predicting the true next word in the training sequence,
using cross-entropy as the loss function.
Recall that the cross-entropy loss measures the difference between a predicted
probability distribution and the correct distribution.
LCE = −
X
w∈V
yt
[w]logyˆt
[w] (10.50)
In the case of language modeling, the correct distribution yt comes from knowing the
next word. This is represented as a one-hot vector corresponding to the vocabulary
where the entry for the actual next word is 1, and all the other entries are 0. Thus,
the cross-entropy loss for language modeling is determined by the probability the
model assigns to the correct next word. So at time t the CE loss in (10.50) can be
simplified as the negative log probability the model assigns to the next word in the
training sequence.
LCE(yˆt
,yt) = −logyˆt
[wt+1] (10.51)
Thus at each word position t of the input, the model takes as input the correct sequence of tokens w1:t
, and uses them to compute a probability distribution over
possible next words so as to compute the model’s loss for the next token wt+1. Then
we move to the next word, we ignore what the model predicted for the next word
and instead use the correct sequence of tokens w1:t+1 to estimate the probability of
token wt+2. This idea that we always give the model the correct history sequence to
predict the next word (rather than feeding the model its best case from the previous
teacher forcing time step) is called teacher forcing.
Fig. 10.18 illustrates the general training approach. At each step, given all the
preceding words, the final transformer layer produces an output distribution over
the entire vocabulary. During training, the probability assigned to the correct word
is used to calculate the cross-entropy loss for each item in the sequence. As with
RNNs, the loss for a training sequence is the average cross-entropy loss over the
entire sequence. The weights in the network are adjusted to minimize the average
CE loss over the training sequence via gradient descent.
26 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
Input
Embeddings
Transformer
Block
Softmax over
Vocabulary
So long and thanks for
Next word long and thanks for all
…
Loss …
…
=
Linear Layer
Figure 10.18 Training a transformer as a language model.
Note the key difference between this figure and the earlier RNN-based version
shown in Fig. ??. There the calculation of the outputs and the losses at each step was
inherently serial given the recurrence in the calculation of the hidden states. With
transformers, each training item can be processed in parallel since the output for
each element in the sequence is computed separately.
Large models are generally trained by filling the full context window (for example 2048 or 4096 tokens for GPT3 or GPT4) with text. If documents are shorter
than this, multiple documents are packed into the window with a special end-of-text
token between them. The batch size for gradient descent is usually quite large (the
largest GPT-3 model uses a batch size of 3.2 million tokens).
10.9.2 Training corpora for large language models
Large language models are mainly trained on text scraped from the web, augmented
by more carefully curated data. Because these training corpora are so large, they are
likely to contain many natural examples that can be helpful for NLP tasks, such as
question and answer pairs (for example from FAQ lists), translations of sentences
between various languages, documents together with their summaries, and so on.
Web text is usually taken from corpora of automatically-crawled web pages like
common crawl the common crawl, a series of snapshots of the entire web produced by the nonprofit Common Crawl (https://commoncrawl.org/) that each have billions of
webpages. Various cleanups of common crawl data exist, such as the Colossal Clean
Crawled Corpus (C4; Raffel et al. 2020), a corpus of 156 billion tokens of English
that is filtered in various ways (deduplicated, removing non-natural language like
code, sentences with offensive words from a blocklist). What is in this data? An
analysis suggests that in large part it’s patent text documents, Wikipedia, and news
sites (Dodge et al., 2021). Wikipedia plays a role in lots of language model training,
as do corpora of books. The GPT3 models, for example, are trained mostly on the
web (429 billion tokens), some text from books (67 billion tokens) and Wikipedia
(3 billion tokens).
10.10 • POTENTIAL HARMS FROM LANGUAGE MODELS 27
10.9.3 Scaling laws
The performance of large language models has shown to be mainly determined by
3 factors: model size (the number of parameters not counting embeddings), dataset
size (the amount of training data), and the amount of computer used for training.
That is, we can improve a model by adding parameters (adding more layers or having
wider contexts or both), by training on more data, or by training for more iterations.
The relationships between these factors and performance are known as scaling
scaling laws laws. Roughly speaking, the performance of a large language model (the loss) scales
as a power-law with each of these three properties of model training.
For example, Kaplan et al. (2020) found the following three relationships for
loss L as a function of the number of non-embedding parameters N, the dataset size
D, and the compute budget C, for models training with limited parameters, dataset,
or compute budget, if in each case the other two properties are held constant:
L(N) = 
Nc
N
αN
(10.52)
L(D) = 
Dc
D
αD
(10.53)
L(C) = 
Cc
C
αC
(10.54)
The number of (non-embedding) parameters N can be roughly computed as follows (ignoring biases, and with d as the input and output dimensionality of the
model, dattn as the self-attention layer size, and dff the size of the feedforward layer):
N ≈ 2 d nlayer(2 dattn +dff)
≈ 12 nlayer d
2
(10.55)
(assuming dattn = dff/4 = d)
Thus GPT-3, with n = 96 layers and dimensionality d = 12288, has 12 × 96 ×
122882 ≈ 175 billion parameters.
The values of Nc, Dc, Cc, αN, αD, and αC depend on the exact transformer
architecture, tokenization, and vocabulary size, so rather than all the precise values,
scaling laws focus on the relationship with loss.2
Scaling laws can be useful in deciding how to train a model to a particular performance, for example by looking at early in the training curve, or performance with
smaller amounts of data, to predict what the loss would be if we were to add more
data or increase model size. Other aspects of scaling laws can also tell us how much
data we need to add when scaling up a model.
10.10 Potential Harms from Language Models
Large pretrained neural language models exhibit many of the potential harms discussed in Chapter 4 and Chapter 6. Many of these harms become realized when
pretrained language models are used for any downstream task, particularly those
2 For the initial experiment in Kaplan et al. (2020) the precise values were αN = 0.076, Nc = 8.8 ×1013
(parameters), αD = 0.095, Dc = 5.4 ×1013 (tokens), αC = 0.050, Cc = 3.1 ×108
(petaflop-days).
28 CHAPTER 10 • TRANSFORMERS AND LARGE LANGUAGE MODELS
involving text generation, whether question answering, machine translation, or in
assistive technologies like writing aids or web search query completion, or predictive typing for email (Olteanu et al., 2020).
For example, language models are prone to saying things that are false, a probhallucination lem called hallucination. Language models are trained to generate text that is predictable and coherent, but the training algorithms we have seen so far don’t have any
way to enforce that the text that is generated is correct or true. This causes enormous
problems for any application where the facts matter!
toxic language A second source of harm is that language models can generate toxic language.
Gehman et al. (2020) show that even completely non-toxic prompts can lead large
language models to output hate speech and abuse their users. Language models also
generate stereotypes (Cheng et al., 2023) and negative attitudes (Brown et al., 2020;
Sheng et al., 2019) about many demographic groups.
One source of biases is the training data. Gehman et al. (2020) shows that large
language model training datasets include toxic text scraped from banned sites. There
are other biases than toxicity: the training data is disproportionately generated by
authors from the US and from developed countries. Such biased population samples
likely skew the resulting generation toward the perspectives or topics of this group
alone. Furthermore, language models can amplify demographic and other biases in
training data, just as we saw for embedding models in Chapter 6.
Language models can also be used by malicious actors for generating text for
misinformation, phishing, or other socially harmful activities (Brown et al., 2020).
McGuffie and Newhouse (2020) show how large language models generate text that
emulates online extremists, with the risk of amplifying extremist movements and
their attempt to radicalize and recruit.
Language models also present privacy issues since they can leak information
about their training data. It is thus possible for an adversary to extract training-data
text from a language model such as an individual person’s name, phone number,
and address (Henderson et al. 2017, Carlini et al. 2021). This is a problem if large
language models are trained on private datasets such as electronic health records.
Related to privacy is the issue of copyright. Large language models are trained
on text that is copyrighted. In some countries, like the United States, the fair use
doctrine allows copyrighted content to be used to build language models, but possibly not if they are used to generate text that competes with the market for the text
they are trained on.
Finding ways to mitigate all these harms is an important current research area in
NLP. At the very least, carefully analyzing the data used to pretrain large language
models is important as a way of understanding issues of toxicity, bias, privacy, and
fair use, making it extremely important that language models include datasheets
(page ??) or model cards (page ??) giving full replicable information on the corpora used to train them. Open-source models can specify their exact training data.
Requirements that models are transparent in such ways is also in the process of being
incorporated into the regulations of various national governments.
10.11 Summary
This chapter has introduced the transformer, and how it can be applied to build large
language models. Here’s a summary of the main points that we covered:
BIBLIOGRAPHICAL AND HISTORICAL NOTES 29
• Transformers are non-recurrent networks based on self-attention. A selfattention layer maps input sequences to output sequences of the same length,
using attention heads that model how the surrounding words are relevant for
the processing of the current word.
• A transformer block consists of a single attention layer followed by a feedforward layer with residual connections and layer normalizations following
each. Transformer blocks can be stacked to make deeper and more powerful
networks.
• Language models can be built out of stacks of transformer blocks, with a linear
and softmax max layer at the top.
• Transformer-based language models have a wide context window (as wide as
4096 tokens for current models) allowing them to draw on enormous amounts
of context to predict upcoming words.
• Many NLP tasks—such as question answering, summarization, sentiment,
and machine translation—can be cast as tasks of word prediction and hence
addressed with Large language models.
• The choice of which word to generate in large language models is generally
done by using a sampling algorithm.
• Because of their ability to be used in so many ways, language models also
have the potential to cause harms. Some harms include hallucinations, bias,
stereotypes, misinformation and propaganda, and violations of privacy and
copyright.


Large Language Models (LLMs) recently demonstrated extraordinary capability in various
natural language processing (NLP) tasks including language translation, text generation, question answering,
etc. Moreover, LLMs are new and essential part of computerized language processing, having the ability to
understand complex verbal patterns and generate coherent and appropriate replies in a given context. Though
this success of LLMs has prompted a substantial increase in research contributions, rapid growth has made it
difficult to understand the overall impact of these improvements. Since a plethora of research on LLMs have
been appeared within a short time, it is quite impossible to track all of these and get an overview of the current
state of research in this area. Consequently, the research community would benefit from a short but thorough
review of the recent changes in this area. This article thoroughly overviews LLMs, including their history,
architectures, transformers, resources, training methods, applications, impacts, challenges, etc. This paper
begins by discussing the fundamental concepts of LLMs with its traditional pipeline of the LLMs training
phase. Then the paper provides an overview of the existing works, the history of LLMs, their evolution
over time, the architecture of transformers in LLMs, the different resources of LLMs, and the different
training methods that have been used to train them. The paper also demonstrates the datasets utilized in the
studies. After that, the paper discusses the wide range of applications of LLMs, including biomedical and
healthcare, education, social, business, and agriculture. The study also illustrates how LLMs create an impact
on society and shape the future of AI and how they can be used to solve real-world problems. Finally, the
paper also explores open issues and challenges to deploy LLMs in real-world scenario. Our review paper
aims to help practitioners, researchers, and experts thoroughly

Language is a vital tool for human expression and communication which we begin to learn after our birth and
The associate editor coordinating the review of this manuscript and
approving it for publication was Bo Pu .
make diverse use of it throughout our lifetime [1], [2].
Nevertheless, machines are unable to possess the innate
ability to understand and speak in human language without
the help of sophisticated artificial intelligence (AI) [3].
Therefore, a long-standing scientific challenge and aim
has been to achieve human-like reading, writing, and
VOLUME 12, 2024

 2024 The Authors. This work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License.
For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/ 26839
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 1. Pipeline of the LLMs training phase.
communication skills in machines [4]. Advances in deep
learning approaches, the availability of immense computer
resources, and the availability of vast quantities of training
data all contributed to the emergence of large language
models (LLMs). LLMs are category of language models that
utilizes neural networks containing billions of parameters,
trained on enormous quantities of unlabeled text data using
a self-supervised learning approach [5]. Frequently pretraining on large corpora from the web, these models
may learn complicated patterns, language subtleties, and
semantic linkages. However, LLMs have proved their ability
in various language-related tasks, including text synthesis,
translation, summarization, question-answering, and sentiment analysis, by leveraging deep learning techniques and
large datasets. Moreover, fine-tuning these models on specific
downstream tasks has been quite promising, with stateof-the-art performance in several benchmarks [6]. LLMs
have their roots in the early development of language
models and neural networks. Statistical approaches and
n-gram models were used in earlier attempts to develop
language models [7]; but these models have shortcomings
in expressing long-term interdependence and context in
language. After that, researchers began to explore more
complex ways with the development of neural networks
and the availability of larger datasets. The creation of
the Recurrent Neural Network (RNN) [8], which allowed
for the modeling of sequential data, including language,
was a crucial milestone. However, RNNs were limited in
their efficacy due to vanishing gradients and long-term
dependencies. The significant advancement in LLMs systems
occurred when the transformer architecture was introduced in
the seminal work [9]. The transformer model is built around
the self-attention mechanism, enabling parallelization and
efficient handling of long-range dependencies. Furthermore,
LLM architectures served as the basis for models such
as Google’s Bidirectional Encoder Representations from
Transformers (BERT) [10] and open AI’s Generative Pretrained Transformer (GPT) series, which excelled at various
language tasks.
 LLMs architecture receives text data from
multiple sources and then the architecture forwards text to
the subsequent stage for preprocessing. It then completes its
training process by executing a series of stages, including
random parameter initialization, numerical data input, loss
function calculation, parameter optimization, and iterative
training. They offer text translation, text summarization,
sentiment analysis, and other services following the training
phase. Prior research has shown the potential of LLMs
in many NLP tasks, including specialized applications in
domains such as the medical and health sciences [11] and
politics[12]. Moreover, after inventing the most sophisticated
GPT model [13], developing the state-of-the-art models
(LLaMa and Bard [14]), and exploring their capabilities, such
as Alpaca and GPTHuggingface [15], LLM has become a
crucial and effective domain. As a result, a trustworthy assessment of current LLMs research is becoming increasingly
important, and prior research has shown the potential and
superiority of LLMs in NLP tasks. Despite this, only a few
studies [3], [16], [17] have thoroughly reviewed latest LLMs
developments, possibilities, and limitations in their research.
Besides, researchers have presented various aspects of
the LLMs domain in several studies [3], [16], [17], [18];
but their work still has several limitations. These studies
miss many aspects of LLM including high-level architecture
and configurations, taxonomies, API and domain-specific
applications, and datasets of LLMs. For example, there
is a lack of introduction to the core architecture and
configurations of the LLMs model, a lack of adequate
explanation of the taxonomy of LLMs, differentiation based
on ML, domain-specific applications, API applications, and
descriptions of LLMs datasets. Furthermore, the vast majority
of LLMs review papers are not peer-reviewed works. The
absence of these key points in a review indicates that a
thorough investigation is missing in the current literature.
Due to the significant extent of the constraints, it is possible
to mitigate these research gaps by thoroughly analyzing and
addressing these missing points. Thus, the motivation of
26840 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 2. Section organization of the review.
this paper is to comprehensively explore the current review
papers, identify their limitations, and outline the current
state-of-the-art methods to address these vital challenges.
Therefore, our primary objective is to explore, comprehend,
and evaluate LLMs that encompass domains, evolution,
classification, the structure of pre-trained models, resources,
and real-time applications. Additionally, our comprehensive
review discusses open issues and challenges associated with
LLMs, including security, ethical, privacy, economic, and
environmental considerations. In addition, we present a set
of guidelines to explore future research and development
in the effective use of LLMs. We hope that this study will
contribute to a better understanding and use of LLMs. The
list of contributions to this paper is as follows:
• Providing a complete overview of LLMs, including their
evolution, classification, and transformer architecture.
The history of LLMs provides a brief account of the
evaluation from its origins (1940) to the present (2023),
as well as a taxonomy of LLMs based on pre-trained and
API-based models and major LLMs structures.
• Describing the comparison of different pre-trained
model designs in LLMs, along with their own systems
that show how the model architectures are different.
• Explaining the influence of ML models on LLMs,
demonstrating the significance of ML in various LLMs
domains.
• Providing a brief overview of the datasets used in the
training phase to differentiate between the models in
existing works.
• Presenting a thorough explanation of the hardware
implementation in training and testing models in terms
of LLMs.
• Defining insights into the potential of LLMs and their
impact on society and demonstrating bio-medical applications in five practical domains, including bio-medical
and healthcare, education, social media, business, and
agriculture.
• Investigating LLMs’s diverse set of open issues, challenges, and future opportunities. This section focuses on
identifying key challenges and future opportunities that
can aid in advancing knowledge in this area.
The growing number of LLMs is an extraordinary development in the field of AI. In recent years, numerous studies [3],
[16], [17], [18] have been conducted to investigate and
evaluate their capabilities. Researchers from various fields
have contributed on the rise of LLMs, shedding light on
their remarkable advancements, diverse applications, and
potential to revolutionize tasks from text generation and comprehension to demonstrating reasoning skills. Collectively,
VOLUME 12, 2024 26841
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 1. Comparison between state-of-the-art research.
these studies contribute to our comprehension of LLMs’
significant role in shaping the landscape of AI-driven
language processing and problem-solving.
Huang et al., [18] presented a study on reasoning in
LLMs that comprehensively summarizes the current state of
LLMs’ reasoning capabilities. It examines various aspects of
reasoning in LLMs, such as techniques to enhance and extract
reasoning abilities, methodologies and criteria for assessing
these abilities, insights from prior research, and suggestions
for future directions. The primary concern is the extent to
which LLMs can demonstrate reasoning skills. This paper
aims to provide an in-depth and up-to-date examination of
this topic, fostering fruitful discussions and guiding future
research in LLMs-based reasoning. In another study, Zhao
et al., [3] survey on LLMs illustrates a comprehensive
examination of the evolution and impact of LLMs in the field
of artificial intelligence and natural language processing.
It traces the historical journey from early language models
to the recent emergence of pre-trained language models
(PLMs) with billions of parameters. Notably, the paper
discusses LLMs’ unique capabilities as they scale in size,
including in-context learning. The authors highlight the
significant contributions of LLMs to the AI community and
the launch of ChatGPT, a prominent AI chatbot powered
by LLMs. The survey is structured around four key aspects
of LLMs: pre-training, adaptation tuning, utilization, and
capacity evaluation. Additionally, the paper provides insights
into available resources for LLMs development and identifies
further research and development areas.
A recent study by Fan et al. [16] conducted a bibliometric
review of LLMs research from 2017 to 2023, encompassing over 5,000 publications. The study aims to provide
researchers, practitioners, and policymakers with an overview
of the evolving landscape of LLMs research. The study
also tracks research trends during the specified time period,
including advancements in fundamental algorithms, major
NLP tasks, and applications in disciplines such as medicine,
engineering, social sciences, and the humanities. In addition
to highlighting the dynamic and rapidly changing nature of
LLMs research, the study offers insights into their current
status, impact, and potential in the context of scientific
and technological advancements. Chang et al. [17] focuses
on the assessment of LMMs. Their research examines the
increasing prevalence of LLMs in academia and industry
due to their exceptional performance in various applications. The study highlights the growing significance of
evaluating LLMs at both the task and societal levels in
order to comprehend potential risks. The paper thoroughly
analyzes LLMs evaluation methods, focusing on three critical
dimensions: what to evaluate, where to evaluate, and how
to evaluate. The research also includes tasks such as natural
language processing, reasoning, medical applications, ethics,
and education. The article examines evaluation methods and
benchmarks for assessing LLMs performance, emphasizing
successful and unsuccessful cases. The paper also underlines
future challenges in LLMs evaluation and emphasizes the
importance of evaluating LLMs as a fundamental discipline
to support the development of more competent LLMs.
Table 1 illustrates the comparison between different review
papers based on some fundamental properties such as LLMs
models, APIs, datasets, domain specific LLMs, ml-based
comparison of LLMs, taxonomy, architectures, performance,
hardware specifications for testing and training, and configurations. Huang et al. [18] lack information on LLMs’ API,
dataset, domain-specific LLMs, taxonomy, architectures, and
LLMs Configurations. In contrast, Zhao et al., [3] has missing
aspects on LLMs’ API, domain-specific LLMs, taxonomy,
architecture, and configurations. Moreover, Fan et al. [16] and
Chang et al., [17] lack information on LLMs’ API, domainspecific LLMs, taxonomy, architecture, and configurations.
On the contrary, our paper offers a considerably broader
aspects on the LLMs context. In addition to incorporating
26842 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
every aspect specified in the table, we provide a detailed
demonstration on the account of the hardware implementation and LLMs datasets. Previous research frequently
focuses on limited aspects of LLMs, including historical
development, bibliometric patterns, and assessment techniques. However, our study recovers previous shortcomings.
A thorough examination is conducted on each of these
aspects, resulting in a comprehensive representation of
the strengths and weaknesses of LLMs. Furthermore, our
research is focused on the crucial element of reasoning
capabilities in LLMs, thereby providing a significant addition
to the body of knowledge in the field. By giving thorough
information, such as descriptions of datasets and hardware
implementations required, our paper stands out as a primary
resource for LLMs practitioners and researchers. Furthermore, we briefly discuss open issues in LLMs research,
such as ethical and responsible AI, multimodal integration,
energy efficiency, privacy and data protection, generalization
and few-shot learning, and cross-lingual and low-resource
settings. We also highlight key challenges, including data
complexity and scaling, tokenization sensitivity, computational resource demands, fine-tuning complexity, real-time
responsiveness, contextual constraints, bias and undesirable
output, knowledge temporality, and evaluation complexity.
Our review suggests future research directions to tackle
open issues and important resource for LLMs researchers
and practitioners. Our extensive systematic review presents
a detailed discussion on LLMs which makes a substantial
contribution to the field of LLMs research.
LLMs refer to a category of AI models developed specifically
to comprehend and produce human language [19]. LLMs
have significantly contributed to the field of AI and
have been applied in diverse areas, including education,
communication, content generation, article composition,
healthcare, research, entertainment, and information dissemination, among others [19], [20]. The origins of LLMs can
be attributed to the emergence and advancement of neural
network-based methodologies in the field of NLP [20].
In order to process language, early NLP systems utilized
rule-based techniques and statistical models. However, those
methods frequently encountered difficulties in comprehending the textual context in a specific discourse [21]. This
section provides a high-level overview of LLMs, including
their background, development, training, and operation.
Figure 3 depicts the history of language models.
In the 1940s, Warren McCulloch and Walter Pitts introduced the idea of artificial neural networks (ANNs) [22].
Afterwards, the 1950s and 1960s saw the development of
the first language models [23]. These models included early
neural networks as well as rule-based models. The processing
of language was facilitated by their utilization of precisely
established linguistic rules and features [24]. These models
experienced limitations in their abilities and encountered
difficulties in managing the complexities of complicated
language assignments. The models were predominantly
employed for tasks involving binary classification. However,
their efficacy in dealing with the complex situation in NLP
tasks was limited [24].
Statistics-based models of language were created in the
’80s and ’90s. These models belong to a category of
models utilized in the field of NLP and machine learning
(ML) with the purpose of capturing and quantifying the
statistical patterns and correlations within language data [21].
Statistical language models have significance in several
applications, such as predictive text input, text generation,
speech recognition, spam detection, etc. These models were
superior in terms of accuracy to early neural networks and
rule-based models, as they were able to process large amounts
of data with ease [21]. Although statistical language models
have been successful in many applications of NLP, they
still have limitation when these models come to predict the
semantic relationship between concepts and context of the
language. These techniques have difficulty dealing with longrange dependencies [25].
During the mid-2000s, the field of NLP witnessed the
introduction of word embeddings, which were recognized as
a notable breakthrough and subsequently acquired considerable attention [26]. Word embedding refers to the process
of representing words in a continuous vector space. The
approach captures the semantic relationships among words
by representing them in a vector space. The representation
reduces the computational cost by mapping the words to a
lower-dimensional space. Word2Vec and GloVe are widely
recognized word embedding models in the domain [27].
These models are mostly utilized for assessing word similarity and assisting in the clustering and representation of
words within semantic domains. Although not classified as
LLMs, these embeddings have significantly contributed to
the progress of natural language comprehension and have set
the path for the development of more complex models. Nevertheless, these models have several limitations, such as their
difficulty in effectively dealing with words that have multiple
meanings (i.e., homonyms) or words that sound the same
(i.e., homophones), as well as their inability to comprehend
contextual information in an accepted manner [26].
The introduction of neural language models in the mid2010s marked a significant advancement in LLMs [28].
These models employed deep learning approaches to acquire
knowledge of language patterns from extensive textual
data and additionally utilized artificial neural networks to
comprehend, produce, or forecast human language. Furthermore, they have demonstrated exceptional outcomes in
a wide range of language-related tasks. The initial neural
language model to be introduced was the recurrent neural
network language model (RNNLM) in 2010 [29]. The
purpose of its development was to capture the sequential
dependencies present in textual data. The utilization of a
hidden state allows for the retention and propagation of
information from preceding words in a particular sequence.
RNNLM has been employed in several applications such
VOLUME 12, 2024 26843
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 3. Brief history of language models.
as text production, speech recognition, machine translation,
and language modeling. The RNNLM demonstrated the
capability to effectively capture the contextual information of
words, resulting in the generation of text that exhibits a higher
degree of naturalness compared to earlier models. Although
the RNNLM offers certain advantages, it is not without its
drawbacks. Some of these limitations include a limited shortterm memory capacity, extended training time requirements,
and prone to suffer in overfitting [30].
In the year 2015, Google unveiled the initial large neural
language model that employed deep learning methodologies.
The technology was referred to as the Google Neural Machine
Translation (GNMT) model [31]. The model underwent
training using huge quantities of multilingual textual data.
This development signifies a notable progression in the field
of machine translation [32]. The model demonstrated exceptional performance on machine translation tasks, departing
from traditional rule-based and statistical techniques in favor
of neural network-based methodologies. When compared
to earlier language models, it was able to tackle complex
natural language tasks with ease. The utilization of this
model resulted in enhanced translation accuracy and the
generation of meaningful translations, while also mitigating
errors associated with intricate linguistic constructions [31].
The advancement of Language models persisted with the
emergence of the Transformer model in the year 2017 [33].
The transformer model has had a significant impact on
the field of NLP and has played a crucial role in the
development of language models such as Bidirectional
Encoder Representations from Transformers (BERT) and
Generative Pre-trained Transformers (GPT) [34]. These
models employ a self-attention mechanism that enables them
to assess the relative significance of individual words in a
sentence, thereby encoding complex relationships within the
text [34]. The primary objective behind the development
of the Transformer model was to overcome the inherent
constraints observed in earlier models such as RNNs and
Long Short-Term Memory (LSTM) networks. The Transformer models possess notable advantages in comparison
to other models due to their ability to capture longer-term
dependencies in language and facilitate concurrent training
on many Graphical Processing Units (GPUs) with a vast
number of parameters, enabling the construction of much
larger models [35]. Parallelization capabilities and scalability
are further benefits that have resulted in notable progress
across many NLP activities [33].
The introduction of BERT in 2018 by Google AI represents
a noteworthy advancement in the domain of NLP [16].
The underlying framework utilized in this study was the
transformer architecture. Before the introduction of BERT,
the preceding language model rooted in NLP had constraints
in understanding contextual information due to its reliance on
unidirectional language modeling. BERT was introduced by
Google as a solution to address this particular constraint [36].
The employed methodology involved the utilization of deep
bidirectional representations, which were conditioned on
both the left and right contexts across all layers [37]. The
pre-trained BERT model was able to undergo fine-tuning by
incorporating an additional output layer, hence enabling its
applicability to diverse tasks such as question answering and
language inference. Due to the widespread adoption of BERT,
several versions and subsequent models, such as RoBERTa,
26844 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
T5, and DistilBERT, have been developed to effectively
address diverse tasks across multiple domains [37].
Following the advent of transformers, subsequent years
saw the development of scaling-up LLMs models through the
expansion of training data and parameter counts[20]. OpenAI
significantly contributed to the development of LLMs in
2018. During the same year, GPT, an additional transformerbased architecture, was developed. Multiple iterations of
the GPT models, developed by OpenAI, underwent pretraining using extensive datasets comprising excerpts from
the Internet, novels, and various other textual sources [38].
The first version of the GPT model was referred to as GPT1 [39]. The introduction of GPT-1 was a notable progression
in the field of NLP. GPT-1 effectively produces words that
are contextually appropriate, showcasing the transformative
capabilities of transformers in significantly advancing natural
language processing tasks. This proficiency is attributed
to its extensive training on a vast number of parameters,
specifically 117 million. The model underwent a two-step
procedure consisting of unsupervised pre-training followed
by supervised fine-tuning [20]. The initial iteration of GPT
did not attain the same level of popularity as BERT due to
several inherent limitations [40]. These drawbacks include a
restricted context window, absence of bi-directionality, and
occasional generation of biased content. Despite the inherent
limits of GPT-1, this model played a crucial role in paving
the way for later, more advanced models. As a result, it has
sparked a new era of AI research and intensified competition
in the development of LLMs.
The subsequent version of the GPT series, known as
GPT-2, was designed with the purpose of addressing the
limitations observed in its predecessor, GPT-1 [40]. Similar
to GPT-1, GPT-2 was developed utilizing the transformer
architecture. In the year 2019, Alec Radford introduced
GPT-2, a language model that was developed on a deep
neural network consisting of 1.5 billion parameters [41].
The GPT-2 model includes a transformer design, which
incorporates self-attention processes to extract information
from different positions within the input sequence. Despite
the high computing cost associated with training and
executing the model, its substantial magnitude facilitates the
comprehension and generation of a wide range of linguistic
subtleties and diversified outputs [40]. The GPT-2 model has
played a pivotal function in the advancement of LLMs and
the execution of NLP activities. The influence of GPT-2 has
had a significant impact on successor models like GPT-3 and
GPT-4, leading to additional advancements in the field of
language processing and creation [42].
In 2019, NVIDIA produced Megatron-LM, which is an
LLMs [43]. Similar to GPT, this model is built on the
transformer architecture. The model possesses a total of
8.3 billion parameters, a notably bigger quantity compared to
the parameter count of GPT-1 and GPT-2 [16]. The magnitude
of this dimension facilitates the model’s capacity to acquire
and produce intricate linguistic structures. Nevertheless,
Megatron-LM has certain limitations, primarily due to
its substantial dimensions, which necessitate substantial
computational resources for both the training and inference
processes [43].
In the year 2020, OpenAI introduced GPT-3 as the
successor to GPT-2 [40]. GPT-3 was trained on an extensive
collection of textual data and demonstrated the ability to
generate text that exhibited a high degree of coherence
and naturalness. Similar to GPT-1 and GPT-2, this model
also utilizes the Transformer architecture [20]. The potential
of LLMs for various NLP applications was exemplified
by GPT-3. This particular LLMs was trained on a deep
neural network with an enormous 175 billion parameters,
surpassing the size of any other LLMs available at that
particular time [16]. The ability to produce natural language
text of superior quality with less fine-tuning is facilitated
by sophisticated methodologies, including a more significant
number of layers and a wider range of training data. One of
the most essential characteristics of GPT-3 is its capacity to
engage in few-shot and zero-shot learning, hence mitigating
the necessity for extensive data in order to generate natural
language text of superior quality. The advent of GPT-3 has
catapulted the domain of natural language processing to new
heights [40]
In the year 2020, OpenAI introduced GPT-4, the subsequent version of their language model, following the
achievements of GPT-3 [20]. Similar to its predecessor,
GPT-4 is a transformer-based model. The system has the
capability to analyze both textual and visual data to produce
textual outputs [16]. The performance of the system was
assessed using a range of standardized professional and
academic examinations specifically intended for human testtakers. GPT-4 exhibited a level of performance comparable to
that of humans on the majority of examinations. Significantly,
it achieved a ranking inside the highest decile of participants
on a simulated iteration of the Uniform Bar Examination [44].
GPT-4 has greater dimension and efficacy compared to
its predecessor, GPT-3, as it possesses the capacity to
generate text that is even more comprehensive and exhibits
a heightened level of naturalness [20].
The development of large language models presents additional prospects for innovation, knowledge acquisition, and
experimentation across diverse domains such as healthcare,
education, research, etc. The utilization of AI and NLP in
these models has significantly transformed how we engage
people with machines.
III. HISTORY OF LARGE LANGUAGE MODELS
LLMs refer to a category of AI models developed specifically
to comprehend and produce human language [19]. LLMs
have significantly contributed to the field of AI and
have been applied in diverse areas, including education,
communication, content generation, article composition,
healthcare, research, entertainment, and information dissemination, among others [19], [20]. The origins of LLMs can
be attributed to the emergence and advancement of neural
network-based methodologies in the field of NLP [20].
In order to process language, early NLP systems utilized
rule-based techniques and statistical models. However, those
methods frequently encountered difficulties in comprehending the textual context in a specific discourse [21]. This
section provides a high-level overview of LLMs, including
their background, development, training, and operation.
Figure 3 depicts the history of language models.
In the 1940s, Warren McCulloch and Walter Pitts introduced the idea of artificial neural networks (ANNs) [22].
Afterwards, the 1950s and 1960s saw the development of
the first language models [23]. These models included early
neural networks as well as rule-based models. The processing
of language was facilitated by their utilization of precisely
established linguistic rules and features [24]. These models
experienced limitations in their abilities and encountered
difficulties in managing the complexities of complicated
language assignments. The models were predominantly
employed for tasks involving binary classification. However,
their efficacy in dealing with the complex situation in NLP
tasks was limited [24].
Statistics-based models of language were created in the
’80s and ’90s. These models belong to a category of
models utilized in the field of NLP and machine learning
(ML) with the purpose of capturing and quantifying the
statistical patterns and correlations within language data [21].
Statistical language models have significance in several
applications, such as predictive text input, text generation,
speech recognition, spam detection, etc. These models were
superior in terms of accuracy to early neural networks and
rule-based models, as they were able to process large amounts
of data with ease [21]. Although statistical language models
have been successful in many applications of NLP, they
still have limitation when these models come to predict the
semantic relationship between concepts and context of the
language. These techniques have difficulty dealing with longrange dependencies [25].
During the mid-2000s, the field of NLP witnessed the
introduction of word embeddings, which were recognized as
a notable breakthrough and subsequently acquired considerable attention [26]. Word embedding refers to the process
of representing words in a continuous vector space. The
approach captures the semantic relationships among words
by representing them in a vector space. The representation
reduces the computational cost by mapping the words to a
lower-dimensional space. Word2Vec and GloVe are widely
recognized word embedding models in the domain [27].
These models are mostly utilized for assessing word similarity and assisting in the clustering and representation of
words within semantic domains. Although not classified as
LLMs, these embeddings have significantly contributed to
the progress of natural language comprehension and have set
the path for the development of more complex models. Nevertheless, these models have several limitations, such as their
difficulty in effectively dealing with words that have multiple
meanings (i.e., homonyms) or words that sound the same
(i.e., homophones), as well as their inability to comprehend
contextual information in an accepted manner [26].
The introduction of neural language models in the mid2010s marked a significant advancement in LLMs [28].
These models employed deep learning approaches to acquire
knowledge of language patterns from extensive textual
data and additionally utilized artificial neural networks to
comprehend, produce, or forecast human language. Furthermore, they have demonstrated exceptional outcomes in
a wide range of language-related tasks. The initial neural
language model to be introduced was the recurrent neural
network language model (RNNLM) in 2010 [29]. The
purpose of its development was to capture the sequential
dependencies present in textual data. The utilization of a
hidden state allows for the retention and propagation of
information from preceding words in a particular sequence.
RNNLM has been employed in several applications such
VOLUME 12, 2024 26843
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 3. Brief history of language models.
as text production, speech recognition, machine translation,
and language modeling. The RNNLM demonstrated the
capability to effectively capture the contextual information of
words, resulting in the generation of text that exhibits a higher
degree of naturalness compared to earlier models. Although
the RNNLM offers certain advantages, it is not without its
drawbacks. Some of these limitations include a limited shortterm memory capacity, extended training time requirements,
and prone to suffer in overfitting [30].
In the year 2015, Google unveiled the initial large neural
language model that employed deep learning methodologies.
The technology was referred to as the Google Neural Machine
Translation (GNMT) model [31]. The model underwent
training using huge quantities of multilingual textual data.
This development signifies a notable progression in the field
of machine translation [32]. The model demonstrated exceptional performance on machine translation tasks, departing
from traditional rule-based and statistical techniques in favor
of neural network-based methodologies. When compared
to earlier language models, it was able to tackle complex
natural language tasks with ease. The utilization of this
model resulted in enhanced translation accuracy and the
generation of meaningful translations, while also mitigating
errors associated with intricate linguistic constructions [31].
The advancement of Language models persisted with the
emergence of the Transformer model in the year 2017 [33].
The transformer model has had a significant impact on
the field of NLP and has played a crucial role in the
development of language models such as Bidirectional
Encoder Representations from Transformers (BERT) and
Generative Pre-trained Transformers (GPT) [34]. These
models employ a self-attention mechanism that enables them
to assess the relative significance of individual words in a
sentence, thereby encoding complex relationships within the
text [34]. The primary objective behind the development
of the Transformer model was to overcome the inherent
constraints observed in earlier models such as RNNs and
Long Short-Term Memory (LSTM) networks. The Transformer models possess notable advantages in comparison
to other models due to their ability to capture longer-term
dependencies in language and facilitate concurrent training
on many Graphical Processing Units (GPUs) with a vast
number of parameters, enabling the construction of much
larger models [35]. Parallelization capabilities and scalability
are further benefits that have resulted in notable progress
across many NLP activities [33].
The introduction of BERT in 2018 by Google AI represents
a noteworthy advancement in the domain of NLP [16].
The underlying framework utilized in this study was the
transformer architecture. Before the introduction of BERT,
the preceding language model rooted in NLP had constraints
in understanding contextual information due to its reliance on
unidirectional language modeling. BERT was introduced by
Google as a solution to address this particular constraint [36].
The employed methodology involved the utilization of deep
bidirectional representations, which were conditioned on
both the left and right contexts across all layers [37]. The
pre-trained BERT model was able to undergo fine-tuning by
incorporating an additional output layer, hence enabling its
applicability to diverse tasks such as question answering and
language inference. Due to the widespread adoption of BERT,
several versions and subsequent models, such as RoBERTa,
26844 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
T5, and DistilBERT, have been developed to effectively
address diverse tasks across multiple domains [37].
Following the advent of transformers, subsequent years
saw the development of scaling-up LLMs models through the
expansion of training data and parameter counts[20]. OpenAI
significantly contributed to the development of LLMs in
2018. During the same year, GPT, an additional transformerbased architecture, was developed. Multiple iterations of
the GPT models, developed by OpenAI, underwent pretraining using extensive datasets comprising excerpts from
the Internet, novels, and various other textual sources [38].
The first version of the GPT model was referred to as GPT1 [39]. The introduction of GPT-1 was a notable progression
in the field of NLP. GPT-1 effectively produces words that
are contextually appropriate, showcasing the transformative
capabilities of transformers in significantly advancing natural
language processing tasks. This proficiency is attributed
to its extensive training on a vast number of parameters,
specifically 117 million. The model underwent a two-step
procedure consisting of unsupervised pre-training followed
by supervised fine-tuning [20]. The initial iteration of GPT
did not attain the same level of popularity as BERT due to
several inherent limitations [40]. These drawbacks include a
restricted context window, absence of bi-directionality, and
occasional generation of biased content. Despite the inherent
limits of GPT-1, this model played a crucial role in paving
the way for later, more advanced models. As a result, it has
sparked a new era of AI research and intensified competition
in the development of LLMs.
The subsequent version of the GPT series, known as
GPT-2, was designed with the purpose of addressing the
limitations observed in its predecessor, GPT-1 [40]. Similar
to GPT-1, GPT-2 was developed utilizing the transformer
architecture. In the year 2019, Alec Radford introduced
GPT-2, a language model that was developed on a deep
neural network consisting of 1.5 billion parameters [41].
The GPT-2 model includes a transformer design, which
incorporates self-attention processes to extract information
from different positions within the input sequence. Despite
the high computing cost associated with training and
executing the model, its substantial magnitude facilitates the
comprehension and generation of a wide range of linguistic
subtleties and diversified outputs [40]. The GPT-2 model has
played a pivotal function in the advancement of LLMs and
the execution of NLP activities. The influence of GPT-2 has
had a significant impact on successor models like GPT-3 and
GPT-4, leading to additional advancements in the field of
language processing and creation [42].
In 2019, NVIDIA produced Megatron-LM, which is an
LLMs [43]. Similar to GPT, this model is built on the
transformer architecture. The model possesses a total of
8.3 billion parameters, a notably bigger quantity compared to
the parameter count of GPT-1 and GPT-2 [16]. The magnitude
of this dimension facilitates the model’s capacity to acquire
and produce intricate linguistic structures. Nevertheless,
Megatron-LM has certain limitations, primarily due to
its substantial dimensions, which necessitate substantial
computational resources for both the training and inference
processes [43].
In the year 2020, OpenAI introduced GPT-3 as the
successor to GPT-2 [40]. GPT-3 was trained on an extensive
collection of textual data and demonstrated the ability to
generate text that exhibited a high degree of coherence
and naturalness. Similar to GPT-1 and GPT-2, this model
also utilizes the Transformer architecture [20]. The potential
of LLMs for various NLP applications was exemplified
by GPT-3. This particular LLMs was trained on a deep
neural network with an enormous 175 billion parameters,
surpassing the size of any other LLMs available at that
particular time [16]. The ability to produce natural language
text of superior quality with less fine-tuning is facilitated
by sophisticated methodologies, including a more significant
number of layers and a wider range of training data. One of
the most essential characteristics of GPT-3 is its capacity to
engage in few-shot and zero-shot learning, hence mitigating
the necessity for extensive data in order to generate natural
language text of superior quality. The advent of GPT-3 has
catapulted the domain of natural language processing to new
heights [40]
In the year 2020, OpenAI introduced GPT-4, the subsequent version of their language model, following the
achievements of GPT-3 [20]. Similar to its predecessor,
GPT-4 is a transformer-based model. The system has the
capability to analyze both textual and visual data to produce
textual outputs [16]. The performance of the system was
assessed using a range of standardized professional and
academic examinations specifically intended for human testtakers. GPT-4 exhibited a level of performance comparable to
that of humans on the majority of examinations. Significantly,
it achieved a ranking inside the highest decile of participants
on a simulated iteration of the Uniform Bar Examination [44].
GPT-4 has greater dimension and efficacy compared to
its predecessor, GPT-3, as it possesses the capacity to
generate text that is even more comprehensive and exhibits
a heightened level of naturalness [20].
The development of large language models presents additional prospects for innovation, knowledge acquisition, and
experimentation across diverse domains such as healthcare,
education, research, etc. The utilization of AI and NLP in
these models has significantly transformed how we engage
people with machines.
IV. METHODOLOGY
Preferred Reporting Items for Systematic Reviews and
Meta-Analyses (PRISMA) guide is crucial for drafting
review papers as it assists systematic reviews in conducting
transparent meta-analyses, accurately reporting aims and
concluding the study, and ensuring the adequate reliability
and relevance with the findings of the study [45]. Therefore,
this review work focuses on the adoption of PRISMA
VOLUME 12, 2024 26845
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 4. PRISMA flow diagram of the review.
technique in analyzing the design, configurations, applications, and challenges of LLMs.
A. INITIAL SEARCHING
The research materials employed in this study have been
acquired from recognized scientific journals and conferences
from January 2020 to August 2023, conducted through the
Google Scholar platform. A comprehensive selection of
scholarly research articles has been specified, encompassing
various reputable academic sources such as IEEE Xplore,
ScienceDirect, ACM Digital Library, Wiley Online Library,
Springer Link, MDPI, and patents. Initially, 355 papers were
selected based on their relevance to the topic and keyword.
Table 2 describes the identification technique of the materials
from various electronic sources.
B. SEARCHING QUERY AND KEYWORDS
Using the combination of the appropriate search queries
and keywords enlisted in Table 3 helps to perform a proper
literature search. To conduct a thorough search of the
articles for our LLMs-based review work, we encompass the
following terms: ‘‘LLMs AND machine learning OR deep
learning OR models,’’ ‘‘LLMs AND machine learning OR
deep learning OR API,’’ ‘‘LLMs AND machine learning OR
deep learning OR Dataset’’, ‘‘LLMs AND natural language
processing OR NLP’’ and ‘‘LLMs AND machine learning
OR deep learning OR tools.’’ These specific searching
techniques help to extract the eligible and quality research
papers.
C. INCLUSION AND EXCLUSION CRITERIA SET
To acquire the final research papers, PRISMA protocols
and principles were adhered to formulate a standard set of
TABLE 2. Electronic database search.
TABLE 3. Search queries used for the review paper.
Inclusion Criteria (IC) and Exclusion Criteria (EC). The
inclusion criteria define the standards of the paper that need
to be included, while the exclusion criteria eliminate articles
that do not meet the inclusion scope. Thus, this manual
screening process improves the transparency of selection
process. Table 4 presents the inclusion and exclusion criteria
set for the proposed study.
D. PRISMA DIAGRAM
Figure 4 depicts the PRISMA flow diagram utilized in
selecting papers for the study. It also provides the numbers
of included and excluded papers for better understanding.
The diagram begins by identifying articles from electronic
databases using keywords, queries, resulting in 355 papers.
After applying the screening method to exclude duplicated,
low-quality, and irrelevant journal papers, the total number
of papers for review is reduced to 294. Following a thorough
analysis of the titles and abstracts, a total of 207 papers were
selected. The final screening method involves the application
of inclusion and exclusion criteria. Following this process,
a total of 135 papers were ultimately selected for the final
review. The process begins with an extensive collection of
papers and reduces to the final selection that meets the predefined selection criteria for the systematic review.
V. LARGE LANGUAGE MODELS
Large language models (LLMs) refer to a specific type of
AI algorithm that holds the capability to execute a diverse
range of NLP tasks. The most common tasks entail text
generation, text analysis, translation, sentiment analysis,
26846 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 4. Inclusion and exclusion criteria.
question answering, and other related functions. GPT-3,
GPT-4, PaLM, and LaMDA are extensively used transformerbased LLMs models trained on a large amount of textual
data. In terms of architectural properties, these models show
variations in size and depth. For example, GPT-3 generates
parameters of 175 billion, distributed across 96 levels, while
PaLM has an even larger parameter number of 540 billion,
organized across 106 layers. All of these models have distinct
configurations. The configurations of GPT-3 and PaLM
differ in terms of their techniques for generating output.
LLMs have evaluated several datasets within Wikipedia, code
repositories, books, question sets, and social media data. They
have demonstrated their ability to execute diverse activities
successfully. Consequently, LLMs have drawn significant
attention for their effective contribution in different domains,
including education, healthcare, media marketing, and other
customer services. A particular LLMs program has superior
performance in a specific domain compared to others, such
as GPT-3, which has gained recognition for its proficiency in
generating text styles, whereas LaMDA demonstrates superior performance in providing accurate responses to factual
inquiries. LLMs are an emerging technological innovation
that holds the potential to bring about transformative changes
across various sectors.
A. BACKGROUND OF LARGE LANGUAGE MODELS
In this section, we present the essential aspects associated.
LLM research requires a comprehensive explanation of the
crucial concept. Various vital aspects, such as tokenization,
encoding technique, layer normalization, etc., are encompassed in the following background section.
1) TOKENIZATION
The primary emphasis is on tokenization, a crucial preprocessing stage of LLMs that involves parsing text into discrete
parts referred to as tokens [46]. Characters, subwords,
symbols, or words may serve as tokens, contingent upon
the language model’s dimensions and nature [47], [48].
Various tokenization algorithms are utilized in LLMs, such
as WordPiece, UnigramLM, and Byte Pair Encoding (BPE).
This algorithm has distinct technique for tokenizing from the
input and then, applied for the specific tasks [47], [48], [49].
2) ATTENTION MECHANISM
The attention mechanisms used in LLMs is a crucial topic
hence it contributes in the improvement of the architecture
and performance. This mechanism helps to figure out the
representation of input sequences by forming links between
various tokens. There are several attention mechanism
available namely Self-Attention where all the queries and
values come from the same encoder-decoder block. Then,
Full Attention which is the naive understanding version of
self attention, and finally, when the output of encoder block
is used as the query of immediate decoder block, is called as
cross attention mechanism [9], [50].
3) ACTIVATION FUNCTION
The activation functions play a vital role in the curve-fitting
capacities of LLMs architectures [51]. Several activation
functions, such as ReLU, GeLU, and other GLU variations,
are explored to determine their performance in current
research on LLMs [52], [53].
4) NORMALIZATION LAYER
Layer normalization is essential for achieving faster convergence in LLMs model and emphasizes their effects on stability during training sessions. It presents different approaches,
such as LayerNorm, DeepNorm, and RMSNorm. These
layer normalization techniques offer distinct advantages and
contribute to the regularization of LLMs applications like
GPT-3, BERT, T5, etc., facilitating effective training [54].
5) TRAINING METHODS AND FRAMEWORKS
LLMs training has different distributed methodologies,
including data parallelism, pipeline parallelism, tensor parallelism, model parallelism, and optimizer parallelism [43],
[55]. These techniques contribute to understand the practical
and expandable training. Additionally, different libraries and
frameworks, including Transformers, DeepSpeed, PyTorch,
TensorFlow, MXNet, and MindSpore, are used frequently for
their training and further implementation [55].
6) DATA PREPROCESSING
The approaches used to preprocess data focus on the
significance of quality filtering, data de-duplication and
privacy reduction in preparing training data for LLMs.
The filtering technique helps to reduce low quality and
relevant data. Besides, it reduces the compute complexity
by ignoring the useless pattern of the input. Duplicate
samples are removed using de-duplication technique which
also avoids the overfitting tendency of the model. Finally,
privacy reduction ensures the security and compliance
of data and upholds the preservation of the personal
data.
VOLUME 12, 2024 26847
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 5. Background of LLMs.
7) PARAMETER TUNING
The researchers explore the many stages of adaptation for
LLMs, starting from pre-training and progressing to finetuning for subsequent tasks. These approaches serve as a
guide for customizing models to suit specific applications.
Several model adaptation and parameter-efficient tuning
techniques, such as prefix tuning, prompt tuning, and adapter
tuning, provide strategies for achieving effective fine-tuning
while minimizing resource usage [56], [57], [58].
This background part aims to provide a thorough understanding of the underlying concepts and approaches that
form the basis of Language Models, which are constantly
developing.
The transformer is employed in most advanced LLMs as
the basic building block because its architecture, scalability,
and pretraining approach enable the model as optimal
framework for constructing robust LLMs. In addition, the
self-attention mechanism of transformers performs effectively for capturing and representing long-range relationships
in language. Consequently, Transformer-based LLMs have
significantly improved the state-of-the-art achievement in
NLP related tasks. In the section V-A1, a comprehensive
overview of transformer architectures, configurations are
provided for building a high-scalable, optimized and costefficient LLMs. Figure 5 depicts the visualization of the
LLMs background.
8) WHAT IS TRANSFORMER?
Transformer architecture is considered as the basic building
block of LLMs. It is intended for neural networks to
efficiently handle sequential data [9]. This architecture does
not use iteration methods. Instead, it employs a focused (i.e.,
attention based) approach to determine global input-output
dependencies. The model can take input of varying lengths
and can change its focus depending on the length of the
sequence. As a result, it has become the go-to architecture
in many fields, often replacing sophisticated recurrent or
convolutional neural networks with much more efficient
structure [59]. In this regard, it is particularly important for
LLMs applications. Figure 6 illustrates the architecture of
the transformer model. Transformer architecture consists of
seven main components. A demonstration of each component
is shown below.
• Inputs and Input Embeddings
The ML models utilize tokens, which are units of
text like words or sub words, as the training data.
However, these models process numbers. Tokenization
begins this translation process by dividing down input
text into meaningful components. A unique number
identification is assigned to each token, connecting
the linguistic information to the numerical vector. This
numerical format is known as ‘‘input embeddings.’’
These input embeddings are numerical representations
of words, which ML models may subsequently process.
These embeddings function similarly to a dictionary,
assisting the model in understanding the meaning of
words by arranging them in a mathematical space where
comparable phrases are situated close together. The
model is trained to generate these embeddings so that
vectors of the same size represent words with similar
meanings. Figure 6A illustrates the input and input
embeddings.
• Positional Encoding
The sequence of words in a sentence frequently
conveys important semantic information. The same
set of words in a different order conveys completely
different meanings. In this regard, understanding the
word order in a sentence is essential in NLP to identify
the correct utterance meaning. In general, in terms of
neural networks, they do not perceive the order of inputs.
26848 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 6. Architecture of a Transformer model.
To address the problem, positional encoding is used to
encode the position of each word in the input sequence
as a collection of integers. The transformer model uses
integer, input embedding and positional encoding to help
GPT in understanding sentence word order and provide
grammatically accurate and semantically appropriate
output [60]. The positional encoding part is shown in
Figure 6B.
• Encoder The encoder is a crucial component of the
neural network which is responsible for processing the
input text. Its primary function is to generate a series of
hidden states that represent the input text in a meaningful
way [61]. Then, it uses a series of self-attention layers
that are often referred to metaphorically as ‘‘voodoo
magic,’’ emphasizing their complex and powerful ability
to capture relationships between different elements in
the input text. In the transformer, the encoder is used
in more than one layer. This section is depicted in
Figure 6C comprehensively.
• Outputs (shifted right) During the training process,
the decoder in the transformer model learns to predict
the next word in a sequence by analyzing the preceding
words. This is achieved through a mechanism known as
autoregressive training. The decoder’s ability to predict
the next word is critical for generating coherent and
contextually relevant sequences. Additionally, the GPT
(GPT-3) is also trained on a massive amount of text
data, that helps it to generate sense while writing any
content. Besides, several corpus including the Common
Crawl web corpus, the BooksCorpus dataset, and the
English Wikipedia are also used during the common
issue. Figure 6D highlights the transformer’s outputs
(shifted right) module.
• Output Embeddings
Input embeddings, which contain text are not directly
recognized by the model. Therefore, the output must
be converted to a format known as ‘‘output embedding.’’ Similar to input embeddings, output embeddings
undergo positional encoding, enabling the model to
understand the order of words in a sentence [62].
In machine learning, the loss function evaluates the
difference between a model’s prediction and the objective value. Loss functions are essential for complex
GPT language models. The loss function modifies a
portion of the model to increase accuracy by reducing
the discrepancy between predictions and targets. The
change improves the overall performance of the model.
The loss function is calculated during training, and
the model parameters are modified. In the inference
process, the output text is created by mapping the
predicted probability of each token in the model to
the corresponding token in the vocabulary. The output
embedding part is illustrated in Figure 6E.
• Decoder
The decoder processes both positionally encoded input
and output embeddings. Positional encoding is crucial
for the model to understand the sequential order of
the tokens in both the input and output sequences.
The positional information helps the decoder effectively capture the structure within the sequences. The
decoder has an attention mechanism that helps to
improve the output’s quality by leveraging contextual
information received from the encoder. The primary
function of the decoder is to create output sequences
based on the encoded input sequences. It generates
a sequence of tokens, often representing words or
VOLUME 12, 2024 26849
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 5. Hardware specifications for the LLMs model.
sub-words, as its output. The dependency between the
encoder-decoder in a transformer is significant where
the encoder processes the input sequence based on
the representation, the decoder provides the desired
output sequence. In addition, GPT is a decoder-only
transformer [63]. The decoder part of GPT uses a
masked self-attention mechanism which can process
the input sequence without requiring encoder explicitly.
Figure 6F demonstrates the decoder component of a
transformer.
• Linear Layer and Softmax
The linear layer is a fully connected neural network
layer that transforms the output embedding into a higherdimensional space. This step is required to convert
the output embedding into the original input space.
This transformation enhances the expressiveness of the
representation, allowing the model to capture more
complex patterns and relationships in the data. Besides,
the softmax function generates a probability distribution
for each output token in the developed vocabulary,
allowing us to generate probabilistic output tokens [64].
Figure 6G shows the process by which the features
are propagated through a linear layer, followed by the
activation of the accurate output probability using the
softmax activation function.
B. HARDWARE SPECIFICATIONS FOR LARGE LANGUAGE
MODELS
Understanding the computing resources and training durations needed for various language models is crucial. This
estimation helps us in decision-making when choosing a
model for specific tasks. To choose a model that is appropriate
for a given task, a clear understanding of the training times
and computational resources is mandatory. Table 5 shows
the hardware specifications, number of parameters, training
duration and other configurations of individual LLMs
model.
GPT-3: GPT-3 uses Nvidia A100 GPUs to pre-train on
a large 300 billion token set, generating around 175 billion
parameters [65]. GPT-3 has context learning features which
enables itself to understand the words reasoning, sentence,
and language properly.
BERT: Trained on an unspecified data scale, the BERT
model has a variable number of parameters that depends
on batch size and the corresponding model’s hidden layer
numbers which is around 340 million. Nvidia A100 and
V100 GPUs are used for training, and the length of the
training depends on the scale of the model’s parameters [66].
Contextual learning is incorporated in the model also.
RoBERTa: RoBERTa, an enhanced version of BERT
which has a parameter count of 340 million and conducts pretraining on a specific amount of data. The training process
completed on 6144 TPU v4 units, running for around a
duration of two weeks[67]. The model also contains a context
learning feature.
T5: T5 uses 1024 TPU v3 units and has a number of
11 billion parameters. T5 has been pre-trained over a number
of tokens of 1 trillion [68]. There is no information available
on GPU training time. It also holds the features of contextual
learning which provides a satisfactory result.
PaLM: PaLM produces a substantial number of parameters, around 540 billion, and it manages the pre-training on
a large dataset with a tokens of 780 billion. The pre-training
process is carried out utilizing by 6144 TPU v4 units [69].
The training period extends for 120 days, and the model also
incorporates contextual learning.
LaMDA: LaMDA uses 1024 TPU v3 units during the
training and the model is pre-trained over 768 billion tokens
26850 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
which generates a total of 137 billion parameters [70].
It requires a total of of 57.7 days during training.
GLM-130B: GLM-130B model possesses a total of
130 billion parameters and undergoes pre-training on a huge
amount of dataset with 400 billion tokens. The training was
conducted utilizing 1024 TPU v4 units and the training
session lasts for 60 days [71].
Gopher: Gopher is a language model that has been pretrained over 300 billion tokens and required 4096 TPU
v3 for the experiment. It has a total of 280 billion
parameters [72]. The GPU training period is precisely stated
as 920 hours. Furthermore, the model integrates context
learning to demonstrate an effective outcome.
Jurassic-1: Jurassic is a model with an impressive capacity
of 178 billion parameters. It has been pre-trained on a massive
dataset of 300 billion tokens, utilizing the computational
power of 800 GPUs [73]. No information regarding the
duration of GPU training is available.
MT-NLG: MT-NLG has a huge size of 530 billion
parameters. It has been trained on a massive dataset of
270 billion tokens, utilizing 4480 80GB A100 GPUs [74].
No data regarding the duration of GPU training is available.
The model integrates context learning features also.
LLaMA: LLaMA is a language model with an enormous
capacity with a total of 65 billion parameters. It has
undergone pre-training on a large dataset consisting of
1.4 trillion tokens. This training process was carried out
utilizing 2048 high-performance 80GB A100 GPUs[75]. The
training period is explicitly set to 21 days.
LLaMA 2: LLaMA 2 is equipped with a total of 70 billion
parameters and has performed pre-training on 2 trillion
tokens, utilizing 2000 80GB A100 GPUs [76]. The training
period is set to 25 days, and the model also contains contextbased learning.
Falcon: Falcon, equipped with 40 billion parameters,
undergoes pre-training on a large dataset of 1.3 trillion
tokens[77]. No details regarding the duration of GPU training
and it also have the context learning features.
Chinchilla: Chinchilla is a language model that has
70 billion parameters and has been pre-trained on 1.4 trillion
tokens[78]. There is no details regarding the duration of GPU
training.
OPT: OPT, equipped with 175 billion parameters, conducts pre-training on 180 billion tokens utilizing 992 A100
GPUs with a capacity of 80GB each [79]. No details
regarding the duration of GPU training.
Galactica: Galactica possesses 120 billion parameters and
has undergone pre-training using 106 billion tokens [80].
Details regarding the duration of GPU training are not given.
BLOOM: BLOOM has a remarkable capacity of 176 billion parameters and has undergone pre-training on 366 billion
tokens utilizing 384 80GB A100 GPUs [55]. The training
period lasts for 105 days, and the model incorporates
contextual learning.
PanGU-a: PanGU-a is a language model that has been pretrained on a massive amount of data, specifically 1.1 billion,
employing 2048 Ascend 910 processing units [81]. It has
an impressive parameter count of 207 billion. No details
regarding the duration of GPU training.
Our comprehensive description helps to understand the
hardware specifications and the computational complexity of
each model. The researchers also find an opportunity to know
about the implementation details of these models and can
improve the performance of their studies.
C. DEEP NEURAL NETWORK ARCHITECTURES OF LLMS
LLMs usually employe deep neural networks to understand
and generate new content more accurately. In this section,
we include a summary of various DNN architectures used in
different LLMs based on literature studies and different real
world applications.
1) COMPARISON BETWEEN STATE-OF-THE-ART STUDIES
An LLM is a dynamic model capable of performing various
tasks, such as creating coherent text and summarizing text.
A defining feature of a language model is its ability to
assume the subsequent words from the preceding text. The
deep neural network (DNN) framework is utilized in LLMs
to enhance its performance which is similar to human-like
understanding [3], [82]. LLMs use different DNN models in
their architecture to enhance task performance.
The transformer architecture serves as the basic building
block of all language models. GPT-1, the initial version of
GPT employs the Transformer decoder architecture [66].
In GPT-1 the decoder structure operates independently from
the encoder, therefore eliminating the multi-head attention
and layer norm components that are linked to the encoder.
The pre-trained GPT model consists of 12 transformer blocks,
each with a d(model) value of 768 and a total of 110 million
parameters. GPT-2, the second version of GPT, employs
the transformer decoder architecture like GPT-1 [66]. GPT2 employs 50,257 BPE tokens and ensures that the masked
multi-head component is preceded by the Layer Norm.
In GPT-2, an additional layer norm is included subsequent
to the last block. There are four pre-trained GPT-2 models
available, each with a unique quantity of decoder blocks.
The largest model, which has a d(model) value of 1600 and
48 blocks, comprises a total of 1.5 billion model parameters.
BERT employs the transformer encoder structure, in contrast
to the Transformer decoder structure utilized by GPT-1 and
GPT-2 [83]. Following the final encoder block is composed of
two fully connected output layers separated by a Layer Norm
component. The calculation of the likelihood of each token’s
output depends on both the previous and next tokens, making
BERT a bidirectional language model. The smaller variant of
BERT consists of 12 encoder blocks with a model dimension
of 768 and a parameter count that is approximately equal to
that of GPT. In contrast, the larger variant has 24 encoder
blocks with a model dimension of 1024 and 336 million
parameters [66].
In contrast to encoder-only models such as BERT and
decoder-only models like GPT-1 and GPT-2, T5 pre-train
VOLUME 12, 2024 26851
M. A. K. Raiaan et al.: Review on Large Language Models
with generative span corruption and an encoder-decoder
architecture [84]. T5 models have displayed state-of-the-art
performance on a wide variety of NLP tasks, like GLUE
and SuperGLUE, and are able to expand up to hundreds of
billions of parameters. LLaMA normalizes the input for every
transformer sub-layer rather than the output [75]. To increase
performance, it employs the RMSNorm normalizing function
and the SwiGLU activation function rather than the ReLU.
Single models are utilized by LaMDA to execute multiple
duties. The model architecture is a decoder-only transformer
language model. The Transformer is comprised of 64 layers,
a d(model) value of 8192, gated-GELU as the activation
function, and relative attention the same as T5 LLMs [70].
AlphaCode employs an encoder-decoder transformer architecture in which input tokens are passed to the encoder, and
one token is extracted from the decoder until an end-of-code
token is generated [85]. When contrasting encoder-decoder
architectures with decoder-only architectures, the encoderdecoder architecture provides the advantage of enabling
bidirectional description representation and provides additional flexibility by separating the encoder structure from
the decoder. It employs an asymmetric architecture with
1536 encoder tokens but only 768 decoder tokens. It makes
use of multi-query attention to lower sampling costs. Cache
update costs and memory utilization are greatly reduced when
all query heads are used but only shared for key and value
heads in each attention block. It employed a SentencePiece
tokenizer for tokenization, trained on a combination of
CodeContests and GitHub data, with a vocabulary size of
8,000 tokens. Through the usage of DNNs, all of these LLMs
have demonstrated remarkable performance on various NLP
tasks like as language understanding and generation.
2) APPLICATIONS OF LLMS USING VARIOUS DNN MODELS
Pre-training Transformer models have led to the proposal
of LLMs with impressive capacities in addressing a variety
of NLP tasks, including question-answering, document
summarization, and language translation [3]. Due to their
remarkable abilities in basic tasks of language processing
and creation, they have completely transformed the fields
of NLP and AI. Various DNN models have been employed
in different industries, such as technology, healthcare, and
retail to increase performance. DNNs have made substantial
progress in improving the capabilities of LLMs [87]. DNN
models, such as convolutional neural networks (CNNs),
recurrent neural networks (RNNs), generative adversarial
networks (GANs), capsule networks (CapsNets), transformers, and BERT, have been extensively employed in diverse
applications of LLMs [94]. Numerous studies [86], [87],
[88], [89], [90], [91], [92], [93] suggest that DNN models
are utilized in several types of LLMs-based applications to
increase task efficiency.
Koizumi et al., [86] introduce an innovative method to
address the issue of insufficient training data in audio
captioning that utilizes a pre-trained LLMs that uses a
decoder for generating captions. The findings of the study
demonstrate the effectiveness of the proposed methodology in
utilizing LLMs for audio captioning. The performance of this
proposed approach outperforms the traditional approaches
which are trained from the scratch.
In a recent study, Fan et al., [87] discuss the significance
of recommender systems in web applications and the
shortcomings of current DNN approaches in predicting user
preferences. They discuss the capacity of LLMs to tackle the
challenges in a recommender systems.
Bai et al. [88] developed an end-to-end non-autoregressive
speech recognition model namely LASO (Listen Attentively
and Spell Once) to improve the speed of inference by simultaneously predicting all tokens. The proposed model utilizes
attention methods to combine decoded speech information
into hidden representations for every token. Moreover, they
suggest using cross-modal transfer learning to increase the
performance of the speech-modal LASO model by utilizing
a text-modal language model to align the semantic meaning
of tokens.
Sun et al., [89] provide a new methodology to predict the
effect of news releases and to minimize potential negative
consequences by automatically forecasting responses in news
media. By utilizing an LLM which utilizes a deep neural
network, their method creates a belief-centered graph on
an existing social network to analyze social dynamics.
The proposed framework shows a satisfactory efficiency in
predicting responses.
Drossos et al., [90] present a technique that enables
an RNN to acquire LLMs for sound event detection. The
proposed approach adjusts the input of the RNN based on the
activity of classes in the preceding time step. This proposed
approach is evaluated on three distinct datasets: the TUT-SED
Synthetic 2016, TUT Sound Events 2016, and TUT Sound
Events 2017 datasets.
Chiu et al. [91] present an efficient method called TPBERT
(based on BERT) for improving the reranking of N-best
hypotheses in automatic recognition of speech. This approach
uses task-specific topic information to increase the BERT
model’s ability to create accurate embeddings of the N-best
hypotheses.
Elhafsi et al., [92] propose a monitoring methodology that
utilizes LLMs to tackle the issue of semantic irregularities in
robotic systems. The efficiency of LLMs-based monitoring
in recognizing semantic abnormalities and aligning with
human thinking is demonstrated through tests on autonomous
driving.
Shen et al., [93] present a self-regulating edge AI
system that utilizes a deep neural network that can plan
automatically, and adjust itself to fulfill the needs of users.
The proposed system uses a hierarchical design known as
cloud-edge-client, where the primary language model is
located in the cloud. By leveraging the robust capabilities
of GPT in language comprehension, and code creation, they
introduce a methodology that effectively handles edge AI
models to meet users’ requirements while automatically
26852 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 6. Comparison of applications of LLMs using various DNN models.
generating new codes for training new models through edge
federated learning.
Table 6 gives a brief overview of these DNN applicationsoriented studies where they applied LLMs. These studies
suggest that employing deep neural networks in language
models increases the performance of LLMs-based applications in several industries..
D. ARCHITECTURAL OVERVIEW OF LARGE LANGUAGE
MODELS
In this subsection, we present a detailed overview on the
architecture of LLMs. Table 7 presents a description and
architecture of LLMs such as GPT-1, BERT, RoBERta, and
T5. The table assists researchers in selecting the optimal
model for a NLP task. GPT-1, BERT base, and BERT
large contain 12, 12, and 24 layers, respectively, in LLMs.
RoBERta is an enhanced variant of BERT, while T5 is
a decoder and encoder transformer. Diagram illustrating
BERT’s input token processing, context-aware embedding,
and masked language modeling tasks, where the masked
words are intended to predict the model. T5 demonstrates
the sequential layers of the transformer model, including the
feedforward neural network, and self-attention. T5 explains
how information flows and structures text. GPT-1 passes data
input embedding and positional encoding through multiple
transformer layers.
E. COMPARISON BETWEEN CONFIGURATIONS OF LLMS
Table 8 provides an extensive overview of various LLMs,
highlighting their configuration details and optimization
settings. These LLMs have played a crucial role in advancing
natural language understanding and generation tasks, making
them a key research topic in NLP. This analysis compares
and contrasts these LLMs based on critical parameters,
including model size, learning rate, category, activation
function, batch size, bias, number of layers, optimizer,
number of attention heads, hidden state size, dropout rate,
and maximum training context length. GPT-4 considered as
one of high performing LLMs with a staggering 1.8 trillion
parameters. It is comparatively faster than the prior GPT
versions and provide many advanced features. Besides, it has
fast response system, generate more accurate output and it
has reduced the biases presented in the model substantially.
GPT-1, despite being lesser with 125 million parameters,
demonstrates the significant development of LLMs over
the years. An increased number of parameters in LLMs
enhances the model’s ability to comprehend intricate patterns
and produce text that is more contextually appropriate and
reminiscent of human language. GPT3’s selection of a
modest learning rate of 6 is notable, which highlights the
significance of cautious hyperparameter selection. Models
are categorized as Causal decoder (CD), Autoregressive
(AR), Encoder-decoder (ED), and Prefix decoder (PD) to
illustrate architectural diversity. Activation functions vary,
influencing the models’ expressive strength from GeLU in
GPT-3 to SwiGLU in LLaMA and LLaMA-2. All versions
of GPT employ the GeLU as its activation function as it
mitigates the vanishing gradient problem and facilitates the
generation of smoother gradients throughout the training
process. The utilization of SwiGLU as the activation function
is observed in models such as PaLM and LLaMA versions
1 and 2, as it has gating mechanisms that enhance its ability
to capture intricate correlations within the data. Models like
BERT, OPT, and T5 use ReLU as the activation function. The
Formula of these activation functions are given below [6],
[59]:
ReLU(x) = max(0, x) = f (x) =
(
x, if x ≥ 0
0, if x < 0
(1)
GeLU(x) = 0.5x(tanh[
p
2/π(x + 0.44715x
3
)]) (2)
SwiGLU(x) = x.Sigmoid(βx).xV (3)
BARD is recognized for its informative response. It features 24 attention heads and facilitates its contextually related
VOLUME 12, 2024 26853
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 7. Architectural overview of different LLMs.
response. BERT size is identical to BARD of 340M. The
key advantage of BERT is understanding the context of
words. It has effective training settings with a proper learning
rate, batch size, and a dropout value of 0.1, leverages the
convergence of the model, and contributes to the NLPbased tasks precisely. PanGU BLOOM, Galactica, and
26854 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 8. Various LLMs with configuration details and optimization settings (Here, LR = learning rate, CG = Category, AF = the activation function, bs =
batch size, NL = the number of layers, NAH = the number of attention heads, SHS = the size of the hidden states, MCLDT = the maximum context length
during training, CD = causal decoder, ED = encoder-decoder, PD = prefix decoder, and AR = autoregressive).
[]
Chinchilla are also LLMs but possess distinct configurations
and challenges. Usually, PanGU is highly effective for the
Chinese language, whereas Galactica performs well with
repeated data. Chinchilla is a scaling strategy constrained by
data limitations and creates efficient resource allocation for
training and generating output. Falcon and T5 are compact
compared to other LLMs, and both are transformer-based
models. However, they have some unique differences, such
as Falcon is a decoder-based model whereas T5 integrated
both encoder-decoders. Additionally, Falcon utilizes multihead query attention to increase the scalability of the model.
LLaMA-2 is the updated version of LLaMA. It is an enhanced
fine-tuned version that exploits the hardware utilization
for efficient training sessions. MT-NLG and PaLM have
substantial parameter sizes of 530B and 540B, respectively.
Both of them also use the casual decoder technique. However,
they have some architectural differences, such as PaLM
uses a SwiGLU activation function and adafactor optimizer.
Moreover, it uses a higher learning rate and batch size of
1 × 102 and 1000K. On the contrary, MT-NLG uses a lower
learning rate and batch size of 5 × 105 and 64K, respectively.
GLM-130B and LaMDA are also effective LLMs, widely
used for NLP-based tasks, including question answering, text
generation, etc. Both of them use the Gated GLU (GeGLU)
activation function, a GLU variant. The following equation is
used to express the GeGLU operation [99].
GEGLU(x, W, V, b, c) = GELU(xW + b) ⊗ (xV + c) (4)
However, there are noticeable differences between GLM130B and LaMDA in terms of their decoder mechanisms.
GLM-130B employs a prefix decoder, whereas LaMDA
adopts a casual decoder technique. In addition, the GLM130B model employs a larger batch size compared to the
LaMDA model. In addition, the presence or absence of
biased terms in models, such as Falcon, T5, LLaMA 1,2,
and Galactica’s ‘‘No,’’ highlights the complexity of the
choices made. From 12 for GPT-1 to 118 for PaLM, the
number of layers affects a model’s ability to capture intricate
patterns. Optimizers are also diverse, with Adam, AdamW,
and AdaFactor playing crucial roles. All GPT variants employ
Adam as the optimizer, although models such as Galactica,
OPT, and Falcon utilize AdamW as their optimizer. Both
T5 and PaLM models utilize the Adafactor optimizer in
their respective architectures. These variations highlight the
significance of selecting models and configurations that are
tailored to particular tasks, with performance, computational
resources, and task requirements playing a central role.
The number of attention heads also exhibits variation
across different models. GPT-1 is equipped with a total
of 12 attention heads, whilst GPT-4 boasts a much larger
number of attention heads, ranging from 120 to 150 within
its model. The additional number of attention heads in the
LLMs enables the model to concurrently attend to several
segments of the input sequence, hence expediting the model’s
training process. In order to enhance the efficacy of the
LLMs, researchers employ diverse dimensions for the hidden
states within their model. The larger dimensions of the hidden
state enable the capturing of complex patterns within the
text. Both GPT 4 and MT-NLG employ hidden state sizes
of approximately 20,000, which is significantly greater in
comparison to the hidden state sizes of other LLMs included
in the table. Certain LLMs models incorporate a dropout
value of 0.1 to prevent overfitting issues, whereas others
do not employ any dropout value. The maximum context
length denotes the number of tokens that can be remembered
by the model during training. Increasing the size of the
context window boosts the model’s ability to grasp the distant
relationships between the texts. Consequently, the model is
VOLUME 12, 2024 26855
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 9. Dataset for large language models.
able to generate text outputs with a great coherence. Table 8
reports that GPT-4 has the context length of 32768 which is
the maximum among all the LLMs. This substantial length
number indicates the capability of GPT-4 to remember the
more extended token sequence during training. LLaMA-2
obtained the second-highest context length of 4096. Most
of the models have a context length of 2048, meaning
they can handle a maximum of 2048 tokens simultaneously
during the text generation. A few compacted models,
including BARD, BERT, and T5, possess a maximum context
length of 512. This table presents a qualitative architectural
comparison among the most popular LLMs. It also provides
comprehensive knowledge about the configurations, strength
of these models. These variations highlight the significance
of selecting models for the particular tasks considering the
performance, computational resources.
F. COMPARISON BETWEEN DATASETS OF LLMS
Different LLMs utilized different datasets for the training
phase, distinguishing the models from one another. A concise
overview of the datasets is provided in this section. Moreover,
it explicitly exhibits the diverse range of datasets used by
the model since understanding of these datasets facilitates
the development and training of the model and boost
the performance. The datasets used to train various large
language models (LLMs) and their compatibility with each
model are detailed in Table 9.
Table 9 demonstrates that datasets have been divided into
multiple categories: webpages, conversation data, literature,
news, scientific data, and codes. This classification enables
us to comprehend the variety of data sources that contribute
to LLMs training. C4, OpenWebText, and Wikipedia are
examples of datasets that belong to the ‘‘Webpages’’ category.
At the same time, BookCorpus, Gutenberg, CC-Stories-R,
CC-NEWES, and REALNEWS are examples of datasets
that belong to the ‘‘Books and News’’ category. These
categories reflect the richness and diversity of text data used
to train LLMs, including web content, novels, news articles,
scientific literature, and codes.
From the ✓, we observe that LLaMA has been trained
on a wide range of data sources, with significant exposure
to webpages (87%), conversation data (5%), books and
news (2%), scientific data (3%), and codes (5%). Therefore,
LLaMA becomes a versatile model suitable for a wide array
of NLP tasks that involve these mentioned data sources.
In contrast, GPT-3 and AlphaCode have limited data access
of data sources to train their models. GPT-1 and GPT-2
focus on webpages (70%) and books & news (30%) data to
train the model. GPT-3 is proficient with web pages (84%),
literature, and news (16%) but requires additional instruction
with conversation data, scientific data, and codes. Diverse
range of datasets enables the GPT models to generate more
contextual information across various domains. Specifically,
the Webpages, books, and news datasets help to employ
formal and structured language. Besides, GPT models
achieve the capability of responding in an informative and
accurate way.
AlphaCode, as its name suggests, is solely focused on
codes (100%) and does not utilize any other data sources.
This feature uniquely distinguish AlphaCode from other
models and emphasize the significance of this model for
code-based tasks. Bard, Bert, and Pangu models exhibit
identical traits, with each of them concentrating on the
extensive textual data obtained from webpage contents and
books for pretraining the models. Bloom and OPT primarily
emphasize on evaluating data from books and websites, such
as Wikipedia or other online sources. On the other hand,
GLM-130 not only analyzes books and web data but also
incorporates computer code data to provide further technological benefits. LaMDA, Galactica and CodeGen models
use scientific data source for training which advance these
models to adapt the scientific knowledge and terminology.
26856 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
Hence, these model can lead to a more accurate responses
in scientific domains. AlphaCode and GLM-130 are the
models of choice for code-related tasks, whereas LLaMA
and BERT excel in diverse text data applications. Most of
the LLMs such as T5, GPT models, Gopher, GLam, PaLM,
and BLOOM frequently utilize websource data which helps
them to automate various practical tasks such as content
creation, data analysis and virtual chatbot for answering the
question. On the contrary, some models such as Falcon and
different version of GPT models utilize books and news
data facilitates in educational application such as document
summarization, and article writings. The models trained on
scientific data have several use cases in research domain.
In addition, Table 9 provides contextual information of the
datasets to maintain the transparency of the comparison
among models and provide an effective guide to future model
implementation. The ‘‘Size’’ and ‘‘Source’’ columns of the
Table listed the additional information. The size of datasets
ranges from 5GB (BookCorpus) to a huge 800GB (several
datasets), indicating the sheer magnitude of data required
to train these LLMs. The source information reveals when
and where the data were collected, which is essential for
understanding the temporal relevance of the training data and
potential biases. Table 9 provides a multitude of information
regarding the datasets used to train LLMs and how each
model leverages these datasets. This information is invaluable
for NLP researchers, developers, and practitioners, as it
enables them to make informed decisions about which LLMs
to use for specific tasks.
G. PERFORMANCE ANALYSIS OF LLMS
LLMs are models that perform the majority of NLP tasks
and numerous models such as GPT-1 through GPT-4,
Bing, ChatpGPT, and BERT have developed in order to
contribute jointly to the industry and academia. Since in
the literature, we find a scarcity of adequate data pertaining
to LLMs, we present performance outcomes for diverse
tasks to publicly accessible LLMs in Table 10. All GPT
series, including GPT-1, GPT-2, GPT-3, GPT-3.5, and GPT4, are evaluated using a variety of metrics, including the
Stanford question answering dataset (SQuAD), language
model benchmark (LAMBADA), and general language
understanding evaluation (GLUE), as shown in Table 10.
GPT-1 obtains a score of 68.4 on the GLUE, while GPT2, GPT-3, GPT-3.5, and GPT-4 attain scores of 84.6, 93.2,
93.5, and 94.4, respectively. GLUE results indicate that
GPT-4 outperforms prior versions of GPT. The GPT-4, i.e.,
in SQuAD and LAMBDA have scores of 93.6 and 82.4,
respectively. As shown in the table, GPT-4 outperforms its
predecessors in both LAMBDA and SQuAD. As GPT-4
outperforms its predecessors in all three benchmark metrics
and exhibits robust performance, it can be concluded that
GPT-4 is significantly more effective than its predecessors in
tasks involving language understanding and language modeling. The VietNamese High School Graduation Examination
(VNHSGE) English dataset was utilized to analyze various
LLMs, including GPT-3.5, BingChat, and BARD. Based
on the accuracy presented in Table 10, it is evident that
BingChat LLM outperforms the other two models, achieving
an accuracy of 92.4%. LLMs such as ChatGPT and Bing were
evaluated using the average intraclass correlation coefficient
(ICC) values. The ICC value for Bing was 0.975, whereas
ChatGPT has an ICC value of 0.858. The higher mean ICC
value indicates that Bing exhibited robust performance and
consistency in major NLP tasks. Table 10 depicts that, all
of the LLMs mentioned in the table have been analyzed
and tested on multiple performance metrics and datasets
to validate the robustness and reliability of these language
models.
VI. RESOURCES OF LARGE LANGUAGE MODELS
LLMs have a wide range of potential applications and
resources available for their development, deployment, and
utilization. Figure 7 presents an LLM taxonomy that divided
into two main branches: i) pre-trained model-based and ii)
API-based. This taxonomy allows us to explore these two
distinct aspects of LLMs.
A. PRETRAINED MODELS
Pretrained language models play a pivotal role in NLP
tasks due to their ability to encapsulate broad language
understanding and generation skills from diverse text sources.
They offer a substantial advantage by minimizing the
computational resources and data required for fine-tuning
specific tasks. There are some of the most common
pre-trained LLMs models, which have been depicted in
Table 11.
1) GENERATIVE PRETRAINED TRANSFORMER (GPT)
GPT [65] is an influential breakthrough in AI, particularly
in NLP tasks. Developed by OpenAI, GPT leverages the
transformer architecture and extensive pre-training on vast
internet text data to achieve a deep understanding of human
language. This generative model excels at tasks like text generation, translation, question answering, and more, making it
a versatile tool across various NLP domains. GPT’s capacity
to capture intricate language patterns and context, coupled
with its iterative improvements, has profoundly impacted
in academia and industry, revolutionizing the landscape of
language understanding and generation.
2) BERT
BERT [10], short for ‘‘Bidirectional Encoder Representations
from Transformers,’’ is a language model with a distinctive
approach. Unlike previous models, BERT is designed to pretrain deep bidirectional representations from unlabeled text
by considering both left and right context in all layers. This
pre-trained BERT model can be fine-tuned with minimal
adjustments to create cutting-edge models for various tasks
like question answering and language inference, eliminating
the need for extensive task-specific modifications. BERT is
both conceptually straightforward and remarkably effective,
VOLUME 12, 2024 26857
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 10. Accuracy of various LLMs on different datasets.
FIGURE 7. Taxonomy of LLMs.
achieving state-of-the-art results on different NLP tasks.
Notable accomplishments include raising the GLUE score to
80.5% (an impressive 7.7% absolute improvement), boosting
MultiNLI accuracy to 86.7% (a 4.6% absolute improvement),
and significantly improving SQuAD v1.1 question answering
Test F1 to 93.2 (a 1.5 point absolute improvement) and
SQuAD v2.0 Test F1 to 83.1 (a remarkable 5.1 point absolute
improvement).
26858 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 11. Description of LLMs.
In our analysis, we have considered variants of BERT that
are pre-trained on extensive text corpora and possess the
characteristics of LLMs, enabling them to understand and
generate natural language comprehensively. This deliberate
choice ensures that the models we have included in our
study harness the full spectrum of language understanding
and generation capabilities, thereby aligning with the core
objective of our research in exploring the impact and
advancements of LLMs in the field of NLP. Non-LLMs
versions of BERT or those with significantly reduced
model sizes were excluded from our analysis to maintain
consistency and relevance in our investigation.
3) ROBERTA
RoBERTA is another LLM which replicates the BERT
pre-training approach outlined by Devlin et al. [67].
We meticulously assess the influence of various critical
hyperparameters and training data sizes. It’s worth noting
that BERT was initially trained with room for improvement,
yet it can now perform on par with or even surpass the
performance of subsequent models that have been published.
As a result, RoBERTa achieves top-tier results in GLUE,
RACE, and SQuAD evaluations. These outcomes underscore
the significance of design decisions that were previously
overlooked and prompt inquiries into the origins of recently
reported advancements.
4) XLNET
XLNet [107] represents a versatile autoregressive pretraining
approach that achieves bidirectional context learning by
optimizing expected likelihood across all possible combinations. XLNet addresses the constraints of BERT through
its autoregressive design and incorporates insights from
Transformer-XL, a leading autoregressive model. In practical
experiments with consistent conditions, XLNet consistently
surpasses BERT on 20 diverse tasks, frequently by a substantial margin. These tasks encompass question answering,
natural language inference, sentiment analysis, and document
ranking, among others.
5) SPEECH-XLNET
Speech-XLNet [108] is a method for training unsupervised
acoustic models to learn speech representations using a SelfAttention Network (SAN) and subsequently fine-tuning it
within the hybrid SAN/HMM framework. Speech-XLNet
acts as a robust regularizer, encouraging the SAN to
VOLUME 12, 2024 26859
M. A. K. Raiaan et al.: Review on Large Language Models
make inferences by prioritizing global structures through
its attention mechanisms. Moreover, Speech-XLNet enables
the model to explore bidirectional contexts, enhancing the
effectiveness of speech representation learning. Experimental
results on TIMIT and WSJ datasets demonstrate that
Speech-XLNet significantly enhances the performance of
the SAN/HMM system in terms of both convergence speed
and recognition accuracy compared to systems trained from
randomly initialized weights. The model best achieves an
impressive relative improvement of 11.9% and 8.3% on
the TIMIT and WSJ tasks, respectively. Notably, the topperforming system achieves a phone error rate (PER) of
13.3% on the TIMIT test set, which, to the best of our
knowledge, is the lowest PER achieved by a single system.
6) DIALOGXL
DialogXL [109] introduces enhancements to tackle longer
historical context and multiparty structures in dialogues.
Initially, alterations are made to how XLNet manages
recurrence, transitioning from segment-level to utterancelevel, thereby improving its effectiveness in modeling
conversational data. Secondly, the integration of dialogaware self-attention, as opposed to the standard selfattention in XLNet, enables capturing crucial dependencies
within and between speakers. While training the DialogXL,
a comprehensive set of experiments is conducted on four
ERC benchmarks, comparing DialogXL with mainstream
models. The experimental results consistently demonstrate
that DialogXL outperforms the baseline models across all
datasets.
7) T5
T5 (Text-to-Text Transfer Transformer) [84] is a groundbreaking LLM developed by Google Research, revolutionizing NLP tasks. T5’s innovation lies in framing all NLP
tasks as text-to-text tasks, simplifying the NLP pipeline
and unifying various tasks under a single framework. Built
upon the Transformer architecture, T5 utilizes multi-head
self-attention to capture intricate language relationships. Its
extensive pre-training on vast text data, followed by finetuning on specific tasks, empowers T5 to excel in text
classification, translation, summarization, question answering, and more. With consistently state-of-the-art results
across NLP benchmarks, T5 has reshaped the field, offering
researchers and developers a versatile tool for comprehensive
language understanding and generation tasks.
8) BIOGPT
BioGPT [110] is a large-scale language model that was
constructed by the Allen Institute for AI (AI2) with the
explicit purpose of undertaking training on biomedical text.
It was trained on an extensive corpus of biomedical literature,
including PubMed abstracts and full-text articles, and is
based on the GPT architecture. It has been demonstrated
that BioGPT outperforms alternative biomedical language
models across a range of tasks, such as query answering,
relation extraction, and named entity recognition. The pretrained weights of the model are accessible to the public,
enabling researchers to optimize it using their biomedical
text data. BioGPT has the capacity to substantially drive
biomedical research forward by facilitating the analysis of
vast quantities of biomedical text data in a more precise and
efficient manner [111], [112].
In summary, pre-trained LLMs are foundational in NLP,
providing a starting point for various applications without
the need for extensive training from scratch. They are widely
used and have access to advanced language understanding
and generation capabilities. However, responsible use and
ethical considerations are essential when working with these
models to ensure fair and unbiased outcomes.
B. API OF LLMS
In this section, we discuss the APIs of LLMs, which have
been described in Table 12.
Open AI API: The API provided by OpenAI offers access
to GPT models that may be utilized for a wide range of
text-related applications [119]. The API facilitates many
tasks such as coding, question and answer, analysis, and
other related activities. The available models encompass a
spectrum of options, spanning from gpt-4 to gpt-3.5-turbo,
as well as many legacy variants. The Chat Completions API
facilitates interactive dialogues by incorporating distinct roles
such as user, and assistance. The programming language
provides support for function calling, which allows for
the retrieval of structured data. The OpenAI API provides
developers with the capability to leverage advanced modeling
of languages for a diverse range of applications.
Hugging Face: Hugging Face provides a complimentary
Inference API that facilitates the examination and assessment
of more than 150,000 publicly available ML models [120].
It features predictive capabilities, and integration with more
than 20 open-source libraries, and facilitates fast change
between models. The API facilitates a range of operations,
including classification, image segmentation, text analysis,
speech recognition, and other related functionalities.
Google Cloud API: The Cloud-based NLP API developed
by Google provides support for a range of approaches, such as
sentiment analysis, text analysis, entity recognition, and other
text annotations [115]. The functionalities can be accessed by
developers through REST API calls utilizing either the client
libraries or their own custom libraries. Additionally, the API
offers moderation functionalities for the purpose of detecting
potentially sensitive content. Several API exists, and each
possesses distinct features and functions.
Microsoft Azure Language APIs: These APIs support many
activities, including sentiment analysis, text summarization,
and other related tasks [116]. Developers use RESTful
endpoints to include Azure LLMs APIs. Microsoft provides
useful SDKs and code examples in other programming
languages, including Python, Java, etc. to facilitate the
utilization of these APIs.
26860 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 12. Comparison of LLMs APIs.
IBM Watson Natural Language: The IBM Watson API
is a robust tool for investigating and extracting valuable
information from textual data. This API offers developers a
variety of functionalities, encompassing sentiment analysis,
emotion analysis, and additional features [117]. Due to its
provision of multilingual support and a user-friendly API,
this technology enables developers to effectively include
sophisticated text analytics into their programs.
Amazon Comprehend API: The Amazon Comprehend
API is a powerful NLP service provided by Amazon Web
Services [118]. This tool evaluates textual data, allowing the
researchers to acquire significant knowledge, such as entity
recognition, language detection, sentiment analysis, and topic
modeling. Due to its ability to accommodate many languages
and simple integration, the tool displays adaptability in
addressing a range of use cases, including customer feedback
analysis and others. The utilization of this API can prove to
be a significant resource for enterprises’ marketing to extract
practical insights from unstructured textual data.
Facebook AI’s Fairseq: The Fairseq framework developed
by Facebook AI is a comprehensive tool for performing
sequence-to-sequence modeling, specifically designed for
handling LLMs [121]. Fairseq is a well-suited API for
many applications related to analyzing and generating natural
language. The platform provides support for advanced
models such as BERT and RoBERTa, allowing researchers
to perform fine-tuning on these models according to specific
needs.
In this study, we have provided a comprehensive overview
of seven popular APIs in Table 12 that leverage the capabilities of LLMs for the purpose of NLP-based functionalities.
However, the taxonomy revealed the presence of several
other APIs that are associated with text analysis but do
not utilize LLMs. These APIs are TextBlob, TextRazor,
Sapling AI, MonkeyLearn, and Aylien, etc., which utilize
traditional machine learning, statistical methods, and rulebased natural NLP techniques instead of relying on extensive
pre-trained LLMs. Since, the primary focus of this study has
VOLUME 12, 2024 26861
M. A. K. Raiaan et al.: Review on Large Language Models
been on describing the tools that particularly utilize LLMs
for the purpose of advanced text analysis, generation, and
comprehension, we have refrained from discussing these
APIs in depth.
VII. DOMAIN SPECIFIC APPLICATION
Since there are several pre-trained models in LLMs, all
of them are utilized by training or fine-tuned to perform
well-defined tasks maintained by their requirements in
different fields. Numerous research studies have consistently
employed LLMs from the diverse domains such as healthcare,
finance, education, forecasting, and natural language processing. The extensive experiments of different LLMs contribute
to revolutionizing the use of AI across these domains. This
section demonstrates the potential contribution of LLMs
application in different domains. Table 13 illustrates the
major contribution of LLMs in the specific domain, as well
as outline their prospective limitations and future directions.
Bio-Medical and Healthcare: As previously stated, GPT
has several versions, ranging from GPT1 to GPT4. GPT3 is
extremely useful in the healthcare industry since it can be
trained to support customer service with no effort. GPT3 gets
all required information through a conversation rather than
an intake form, and many systems might be built to assist
numerous patients at the same time [126]. Besides, clinics
and hospitals are places to cure illness, but it is also true
that various contagious viruses are brought into these places.
Patients and healthcare providers can be better protected from
infection by replacing a human receptionist with a robot
which becomes increasingly important during the COVID19 epidemic [140]. Since clinics and hospitals often see a
high volume of patients on a daily basis, an optimum and
lightweight system may submit several queries for single
patients to create acceptable output.
Consequently, GPT models can also aid in cost reduction
in the medical industry. Furthermore, biomedical and clinical
text mining has always been an essential and major challenge
due to the complex nature of domain corpora and the
continually expanding number of documents. As a result,
BERT models can improve the performance of biomedical
and clinical text mining models [141]. Salam et al., [128]
and Korngiebel et al., [126] demonstrate the substantial
advantages of ChatGPT in the domains of healthcare, clinical
research, and practice, although simultaneously underscoring
the imperative necessity for proactive inspection and ethical
transparency. Several studies [125], [129], [131], [132]
explore the utilities and constraints of LLMs such as
ChatGPT in the context of clinical practice, research, and
public health. In their study, Kung et al., [130] conducted an
evaluation of ChatGPT’s performance on the United States
Medical Licensing Examination (USMLE), and the outcomes
indicate the potentiality of LLMs to support clinical decisionmaking and medical education. Sorin et al., [124] evaluated
ChatGPT-3.5 as a decision support for breast tumor boards
where they compared the tumor board’s explanations, and
summaries with ChatGPT-3.5 and showed that ChatGPT-3.5
and the tumor board had a high degree of decision alignment.
Huang et al., [123] investigate the prospective applications
of LLMs with a specific emphasis on ChatGPT, in the field
of dentistry, mainly focusing on automated dental diagnosis
and highlighting the efficacy of LLMs in dental diagnosis.
Furthermore, the XLNet contributes to better clinical note
representation by adding temporal information and a realistic
prediction setup [142]. Furthermore, various LLMs models
also assist the medical industry by making the procedure
easier than previously.
Education: Educators have struggled for a long time
with an unequal educational resources to student demand
across disciplines. One of the significant challenges is a
shortage of accessible educational resources for pupils to
study outside of school. Although online instructional videos
are helping to alleviate the problem, society still hopes that
AI will deliver individualized teaching services to satisfy
the learning demands of each student and increase teaching
efficiency. In the light of above discussion, LLMs have the
potential to revolutionize many facets of learning, teaching,
and educational research in the education sector [140].
The GPT model aids the students in converting the math
word problems into representative equations [143]. Kasenci
et al., [19] highlighted substantial impact of LLMs in
education by facilitating personalized learning, automating
grading process, and accessibility of educational resources.
Hadi et al., [137] presents a thorough analysis of LLMs, covering their historical development, wide-ranging applications
in domains such as medicine, engineering, education, and
their potential impact on the trajectory of AI. Lo et al.,
[138] and Dwivedi et. al. [139] investigate the prospective
uses of ChatGpt within the realm of education and identify
the primary obstacles that have arisen during its initial
deployment. Besides, in terms of writing authentic texts in
distinct formats, including essays, summaries, and articles,
these models help to accomplish this without any error.
In contrast, the manual process may have human errors
in the documentation. In this case, the GPT model helps
to address this problem. In addition, the XLNet helps to
understand the texts and documents which can be utilized
in the academic sector [38]. Furthermore, other models may
impact the education system by making it more engaging,
accessible, and productive for both students and teachers.
Social Media: The LLMs have leveraged several aspects
of the social media industry regarding content production,
moderation, sentiment analysis, etc. There are some tasks
in the social media can be generated such as writing
content, classifying text, and even full blogs and articles for
social media. These models can also perform named entity
recognition (NER) and text classification [144], [145]. When
the GPT, XLNet, BERT, etc., models aid the writer and
content producers in generating a consistent flow of write
up. It also provides content suggestions, and to create a
safer online environment, these models are hired to assist
in discovering and filtering out different dangerous and
improper content. Abramski et al., [42] utilized network
26862 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 13. Domain specific machine learning-based study comparison in LLMs.
VOLUME 12, 2024 26863
M. A. K. Raiaan et al.: Review on Large Language Models
TABLE 13. (Continued.) Domain specific machine learning-based study comparison in LLMs.
science and the principles of cognitive psychology to evaluate
biases present in LLMs. Sobieszek et al., [136] presents a
critical examination of the stated semantic capabilities of
GPT-3, aiming to challenge the current view of its dismissal.
Moreover, it assists in determining public opinion on certain
topics by analyzing public interest and demand.
Business: In business, LLMs helps companies improve
their decision-making processes, product manufacturing
processes, operations, and customer interactions. Communicating with customers and providing 24/7 customer service
by answering their queries, assisting them in their work,
and providing advanced advice related to areas of interest to
customers is crucial for business progress. Moreover, it is also
important to analyze customer sentiment, market trends, risk
factors, and competitive intelligence [20]. In this case, LLMs
help to fulfill all their requirements within a short period.
The LLMs models, like GPT, XLNet, BERT, etc., play a
vital role in creating customer documents and product details
and efficiently maintaining the entire business by saving
time and reducing laborious tasks. Frederico et al., [135]
presents an initial investigation into the potential applications
and effects of ChatGPT in the domain of supply chain
management. Their study provides significant insights for
professionals engaged in this domain. Mich et. al. [133]
present an initial investigation of potential hazards associated
with the implementation of ChatGPT in bussiness domain.
Yu et al., [134] presented an analysis of the capabilities
of LLMs, specifically GPT-4, in the context of financial
forecasting for a time series. Besides, their findings reveal
that the performance of LLMs outperforms other traditional
models also.
Agriculture: In agriculture, variations of GPT models,
including GPT3, BERT, and XLNet models, play a significant
role [146], [147], [148]. They are able to analyze large data
hubs of soil, crop, and weather data along with satellite
imagery. These models provide recommendations on planting
26864 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
FIGURE 8. Visual representation of impact on LLMs.
times, irrigation, fertilizer application, and optimizing fields
and resources. Farmers can obtain current updates and market
requirements, predict crop prices, anticipate natural disasters,
and document farmers’ and crop details. Manual agricultural
management can be time-consuming and laborious, but these
LLMs can support to accomplish these tasks to a greater
extent.
VIII. IMPACT OF LARGE LANGUAGE MODELS ON
SOCIETY
LLMs and similar AI technologies have had a profound
impact on society across various domains. The impact
of LLMs on society is multifaceted, and it is important
to consider both the positive and negative consequences.
As these technologies continue to evolve, stakeholders,
including governments, businesses, researchers, and the
general public, must work together to harness the benefits
of LLMs while addressing their challenges and ethical
implications. The visual representation of Figure 8 effectively
demonstrates the impact of LLMs, outlining their benefits on
the left and the adversarial impacts on the right side. The
positive impacts of LLMs are as follows:
• Advancements in Natural Language Processing
(NLP): LLMs have significantly advanced the field
of NLP, making it possible to automate and scale a
wide range of language-related tasks such as translation, summarization, sentiment analysis, and more.
In recent years, Natural Language Processing (NLP) has
witnessed significant advancements, primarily driven
by the emergence of Large Language Models (LLMs).
These advancements, exemplified by models such as
BERT [10], RoBERTa [67], and XLNet [107], have
transformed the NLP landscape. Notably, LLMs have
been fine-tuned for various specific NLP tasks, enabling
remarkable performance improvements. Multilingual
models like mBERT [149] and cross-lingual models like
XLM-R [150] have facilitated language understanding
across diverse linguistic contexts. Additionally, there
has been a focus on creating more efficient versions of
LLMs such as DistilBERT [151] and ALBERT [152].
These developments have not only expanded the
applicability of NLP but have also raised ethical considerations, prompting research in bias mitigation [153] and
responsible AI. LLMs have enabled breakthroughs in
applications like conversational AI, few-shot and zeroshot learning, and domain-specific NLP in fields like
healthcare and finance. These advancements underscore
the pivotal role of LLMs in advancing the capabilities
of NLP and continue to shape the future of language
understanding and generation.
• Automation and Efficiency: LLMs are used to automate
tasks that were previously time-consuming and laborintensive, leading to increased efficiency in industries
such as customer support, content generation, and data
analysis. The automation and efficiency of LLMs, driven
by models like BERT and GPT, have revolutionized
industries and applications. These models have automated intricate language-related tasks, from sentiment
analysis to language translation, making them more efficient and accessible. LLMs, such as DialoGPT [154] and
ChatGPT, have powered conversational AI, streamlining
customer support and interactions. Moreover, they excel
in few-shot and zero-shot learning, as demonstrated by
GPT-3 [155], automating tasks with minimal examples.
Multilingual LLMs like mBERT have automated language tasks across various languages, enhancing global
accessibility. Efficiency has further advanced through
models like DistilBERT and ALBERT, which maintain
performance while reducing computational resources.
These models can be fine-tuned for specific domains,
such as healthcare [156], making them indispensable in
automating domain-specific tasks efficiently.
• Content Generation: LLMs are capable of generating
human-like text, which has implications for content
creation, including automated news articles, marketing
materials, and creative writing.
• Language Translation: LLMs have improved machine
translation systems, making communication across languages more accessible and accurate.
• Virtual Assistants and Chatbots: LLMs power virtual
assistants and chatbots, enhancing customer service and
providing round-the-clock support in various industries.
• Medical and Scientific Research: LLMs are used
to analyze and summarize vast amounts of medical
and scientific literature, aiding researchers in finding
relevant information quickly.
• Accessibility: LLMs have the potential to improve
accessibility by providing real-time translation and
transcription services for individuals with hearing
impairments or language barriers.
• Personalization: LLMs enable personalized recommendations and content curation on platforms such as social
media, e-commerce, and news websites.
• Creative Tools: LLMs are used as creative tools in
various art forms, including generating poetry, music,
and visual art.
VOLUME 12, 2024 26865
M. A. K. Raiaan et al.: Review on Large Language Models
• Education and Skill Development: The rise of LLMs
underscores the importance of education and skill development in AI and data science, as these technologies
become increasingly integral to various industries.
In addition to numerous positive sides, LLMs also entail
some downsides. These downsides are outlined as follows:
• Ethical Concerns: Bias and fairness issues in LLMs
have raised ethical concerns. LLMs may perpetuate or
amplify biases present in training data, leading to unfair
or discriminatory outcomes.
• Misinformation and Disinformation: LLMs can generate realistic-sounding fake text, raising concerns about
the spread of misinformation and disinformation.
• Job Displacement: The automation capabilities of
LLMs may lead to job displacement in certain industries,
particularly in routine data-entry and content-generation
roles.
• Data Privacy: The use of LLMs often involves processing large amounts of user-generated text data,
which raises data privacy concerns, especially regarding
sensitive or personal information.
• Economic Impact: The adoption of LLMs can disrupt
traditional business models and create economic shifts
as industries adapt to automation and AI technologies.
• Regulation and Accountability: Policymakers and
regulators are grappling with the need to establish
guidelines and regulations for the responsible use of
LLMs, including addressing issues of bias, transparency,
and accountability.
IX. INDUSTRIAL SIGNIFICANCE OF LARGE LANGUAGE
MODELS
LLMs have gained substantial popularity in various industries, bringing about radical transformations. Influence of
LLMs in industries is visible which can be presented through
several key facets:
1. Enhancing NLP Applications: LLMs have ushered in
a revolution in NLP applications [157] across sectors like
customer service, chatbots, and sentiment analysis. They
contribute to more precise and efficient interactions with
users, leading to increased customer satisfaction and reduced
response times.
2. Enabling Data Analysis and Information Extraction:
LLMs play a pivotal role in extracting valuable insights
from unstructured text data [158]. This is particularly
critical in fields like finance, market research [159], and
healthcare, where deciphering market trends, sentiment in
news, or medical records hold paramount significance.
3. Facilitating Translation Services: Industries heavily
reliant on multilingual communication [160], such as ecommerce, travel, and international business which may be
benefited from LLMs that streamline automated translation.
Translation service saves resources and ensuring high-quality
translations across multiple languages.
4. Empowering Content Generation: LLMs are harnessed
for content generation [161], which encompasses automated
article writing, social media posts[162], product descriptions,
and more. This automation simplifies content creation
processes and allows for scalable production of top-tier
content.
5. Revolutionizing Healthcare: LLMs find applications in
medical record analysis [129], diagnosis assistance, and drug
discovery. They empower healthcare professionals to access
and comprehend extensive medical literature and patient data,
thereby enhancing healthcare decision-making.
6. Revamping Education: The education sector [163]
leverages LLMs for automated grading, ensuring prompt
feedback to students. These models also contribute to the
development of intelligent tutoring systems and personalized
learning platforms.
7. Aiding Legal Practices: Legal practitioners [164]
benefit from LLMs for contract analysis, legal research,
and document review. These models assist in efficiently
extracting pertinent information and identifying potential
legal concerns.
8. Assisting Human Resources: LLMs support HR
professionals [165] in tasks like candidate screening, resume
parsing, and identifying potential job candidates. They
streamline time-consuming processes within the recruitment
phase.
9. Empowering Financial Services: In the realm of
financial services [166], LLMs come into play for activities
like sentiment analysis of news articles, algorithmic trading,
risk assessment, and fraud detection. They are instrumental in
making informed investment choices and managing financial
risks.
10. Boosting E-commerce: LLMs enable personalized
product recommendations [167], chatbots for customer
support, and efficient inventory management. These enhancements result in enriched user experiences and heightened
sales.
11. Illuminating Customer Insights: LLMs analyze
customer reviews [168], feedback, and social media data, furnishing businesses with insights into customer preferences,
opinions, and sentiments. This invaluable information aids
companies in customizing their products and services.
As LLMs continue to advance, their industrial importance is undeniable. LLMs streamline operations, enhance
decision-making, and bolster efficiency across diverse
domains, positioning them as a transformative technology in
the contemporary business landscape.
X. OPEN ISSUES AND CHALLENGES
This section discusses critical analysis of open issues and
challenges of LLMs.
A. OPEN ISSUES
In this section, we delve into the open issues related to LLMs.
These issues appeared recently as focal point in AI research
and development. We raise the necessity for ongoing research
and innovation to resolve issues that have emerged alongside
the rapid development of LLMs. Our discussion will cast light
26866 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
on the significance of these unresolved issues, highlighting
their impact on various applications and the AI landscape as
a whole.
• Issue 1: Ethical and Responsible AI The question
regarding how to ensure the ethical use of large language
models remains unresolved. Filtering, moderation, and
accountability concerns regarding AI-generated content
remain problematic. Misinformation, hate speech, and
biased content generated by LLMs necessitate continuous research and development [169].
• Issue 2: Multimodal Integration While LLMs are
predominantly concerned with text, there is a growing
demand for multimodal models that can comprehend
and generate content that includes text, images, and
other media types [170]. Integrating multiple modalities
into a single model poses difficulties in data acquisition,
training, and evaluation.
• Issue 3: Energy Efficiency The environmental impact
of training and deploying large language models is still
an urgent concern [171]. It is essential to develop more
energy-efficient training methods, model architectures,
and hardware solutions to reduce the carbon footprint of
LLMs.
• Issue 4: Security and Adversarial Attacks
LLMs are vulnerable to adversarial context, where
slight input modifications can lead to an unexpected
and potentially harmful outputs [172]. Improving model
robustness and security against such situation is a crucial
area of study, particularly for cybersecurity and content
moderation applications.
• Issue 5: Privacy and Data Protection As LLMs
become more competent, user privacy and data protection concerns increase. Finding methods for users to
interact with these models without compromising their
personal information is an ongoing challenge. There is a
need for research on privacy-preserving techniques and
regulatory compliance [173].
• Issue 6: Generalization and Few-Shot Learning
LLMs performs well when there is abundant data
but struggles with tasks requiring few examples or
domain-specific knowledge. Improving their capacity to
generalize and perform well with limited training data is
a crucial area of research [174].
• Issue 7: Cross-Lingual and Low-Resource Settings It
is an ongoing challenge to make LLMs more accessible
and effective in languages and regions with limited
resources and data [175]. Global applications require
developing techniques for cross-lingual transfer learning
and low-resource language support.
B. CHALLENGES
LLMs have rapidly evolved from being non-existent to
becoming a ubiquitous presence in the field of machine
learning within just a few years. Its extraordinary ability
to generate text that resembles that of a human which has
attracted significant attention and applications in numerous
fields. However, this sudden rise of these technological
dependencies with higher impact has also revealed many
challenges and concerns. In this discussion, we will examine
ten of the most significant challenges pertaining to LLMs.
• Challenge 1: Data Complexity and Scale In the era of
LLMs, the size and complexity of the datasets on which
they are trained is one of the most significant challenges.
These models are typically trained on enormous corpora
of Internet-sourced text data. These datasets are so
extensive that it is nearly impossible to understand or
investigate the totality of their information. This raises
concerns regarding the quality and biases of the training
data and the potential for the unintentional dissemination
of detrimental or inaccurate information [176].
• Challenge 2: Tokenization Sensitivity
For analysis, LLMs rely significantly on tokenization, dividing text into smaller units (tokens) [177].
Tokenization is essential for language processing and
comprehension but can also present challenges. For
instance, the meaning of a sentence can alter significantly based on the choice of tokens or the ordering
of words. This sensitivity to input phrasing can lead
to unintended outcomes when generating text, such
as adversarial assaults and output variations based on
minute input changes.
• Challenge 3: Computational Resource Demands
The training of LLMs is a computationally intensive
procedure that requires substantial hardware and energy
resources [178]. It is necessary to have access to
supercomputing clusters or specialized hardware in
order to train large models, and the environmental
impact of such resource-intensive training has raised
concerns. Significant energy consumption is associated
with training LLMs at scale, contributing to the AI
industry’s overall carbon footprint.
• Challenge 4: Fine-Tuning Complexity
While pre-training gives LLMs a broad comprehension
of language, fine-tuning is required to adapt these
models to specific tasks [179]. Fine-tuning entails
training the model on a smaller dataset, frequently
requiring human annotators to label examples. As it
involves the construction of task-specific datasets and
extensive human intervention, this process can be both
time-consuming and costly.
• Challenge 5: Real-Time Responsiveness The remarkable training capabilities of LLMs come at the expense
of inference speed. Real-time response or prediction
generation with these models can be sluggish, limiting
their applicability in applications such as chatbots or
recommendation systems where low-latency responses
are crucial for user satisfaction.
• Challenge 6: Contextual Constraints
LLMs can only evaluate a limited number of preceding
tokens when generating text due to their limited context
VOLUME 12, 2024 26867
M. A. K. Raiaan et al.: Review on Large Language Models
window [180]. This limitation presents difficulties when
working with lengthy documents or having lengthy
conversations. Maintaining coherence and relevance
over lengthy text sequences can be challenging because
the model may neglect or lose track of the relevant
information.
• Challenge 7: Bias and Undesirable Output
In the output, LLMs display biases or undesirable
characteristics. This is due to the inherent biases in
the training data, which are assimilated by the model
and reflected in its responses [181]. Such biases can
manifest as objectionable, discriminatory, or harmful
content, making it imperative to address and mitigate
these concerns to ensure the responsible deployment of
AI.
• Challenge 8: Knowledge Temporality
LLMs learn using historical data from the Internet, and
their knowledge is restricted to what is available as of
a particular date. Consequently, they may lack access
to the most recent information or events. This can be
problematic when users expect up-to-date responses or
when the conversation involves recent events.
• Challenge 9: Evaluation Complexity
Evaluation of LLMs presents significant difficulties.
Many extant evaluation metrics are insufficient to
capture the nuances of model performance, which
raises questions about their efficacy. Additionally, these
metrics can be susceptible to manipulation or gaming,
which may provide an inaccurate image of a model’s
capabilities. To assess LLMs’ actual performance and
limitations, robust and reliable evaluation methodologies are required.
• Challenge 10: Dynamic Evaluation Needs
Frequently, evaluating LLMs entails comparing their
outputs to static benchmarks or human-authored ground
truth. However, language is dynamic and evolves, and
preset evaluation data may not adequately reflect a
model’s adaptability to language and context change.
This difficulty underscores the need for evaluation
frameworks that are more dynamic and continually
updated.
XI. FUTURE RESEARCH PROSPECTS ON LLMS
Since LLMs are emerging research topic in recent times,
several key research focuses and directions are prominent
that may address and resolve the challenges and open issues
discussed earlier. Resolving these open issues and challenges
may harness the full potential of LLMs while ensuring its
responsible and ethical use in AI landscape.
A. ENHANCING BIAS MITIGATION
Researchers are dedicated to refining training data to
minimize bias, devising effective debiasing techniques, and
establishing guidelines for responsible AI development[182].
They also need focus on integrating continuous monitoring
and auditing mechanisms into AI pipelines, thereby conforming fairness and impartiality of the system. This commitment
to mitigating bias ensures that LLMs not only advance in
capability but LLMs also upholds ethical standards.
B. EFFICIENCY OPTIMIZATION
A core concern driving research is the quest of efficient
training techniques. Researchers are delving into innovative
methods like federated learning, which enables the distribution of training across decentralized data sources [183].
They are also exploring knowledge distillation techniques
for model compression and finding ways to reduce the
substantial computational and environmental costs associated
with LLMs. This optimization paves the way for more
sustainable and resource-efficient AI models.
C. DYNAMIC CONTEXT HANDLING
LLMs are being endowed with enhanced context management capabilities. This empowers them to comprehend longer
context windows and seamlessly handle extensive documents
or conversations. Such enhancements significantly expand
their utility in various applications and resolve previous
limitations.
D. CONTINUOUS LEARNING
To keep LLMs up-to-date, researchers are focusing on
developing techniques that enable these models to adapt
on evolving language and knowledge over time. This
ensures that LLMs remain valuable and accurate sources of
information and consistently overcoming challenges of being
outdated.
E. INTERPRETABLE AI
The research community is committed to making LLMs’
outputs more transparent and interpretable. Improving interpretability fosters the confidence and comprehension in AI
decision-making processes which has been a major concern
for a long time after the advent of LLMs [184].
F. MULTIMODAL LLMS
Researchers are pioneering the development of LLMs that
incorporate text, vision, and other modalities [185]. These
models can understand and generate text from images, videos,
and audio, creating new avenues for AI applications and
effectively addressing the need for multi-sensory comprehension.
G. HUMAN-AI COLLABORATION
Research on how humans and LLMs can collaborate
effectively, with AI assisting and augmenting human tasks,
is a crucial focal point. This collaboration bridges the gap
between AI capabilities and human needs, thereby resolving
previous challenges and issues in deployment.
26868 VOLUME 12, 2024
M. A. K. Raiaan et al.: Review on Large Language Models
H. DYNAMIC EVALUATION METRICS AND RELEVANT
BENCHMARKS
Researchers are working on dynamic evaluation metrics that
adapt to changing language and context, ensuring that LLMs
performance is accurately assessed [186]. Finding a suitable
metric along with the development of relevant and up-todate benchmarks which may address earlier shortcomings in
assessing AI capabilities.
I. PERSONALIZATION AND CUSTOMIZATION
Techniques to customize LLMs interactions to individual user
preferences and needs are gaining popularity nowadays. This
personalization boosts user satisfaction and resolves issues
related to one-size-fits-all AI interactions.
J. ETHICAL AND LEGAL FRAMEWORKS
In response to evolving AI regulation, researchers are
diligently developing ethical and legal regulatory frameworks. These frameworks serve as guiding principles
for the responsible use of LLMs and ensure compliance with data protection and privacy regulations,
effectively addressing previous concerns about ethical AI
deployment [187].
These future research directions may overcome longstanding challenges and open issues raised in LLMs domain. These
avenues may lead to the maximization of LLMs potential by
the future researchers while upholding the highest standards
of accountability and ethics in AI landscape.
